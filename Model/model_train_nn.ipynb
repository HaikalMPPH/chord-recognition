{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a087c41a-4373-4ec1-972b-e8e8899669ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":::::::::::::::::::: PREPARING ::::::::::::::::::::\n",
      "[INFO]: Featurizing 45 files: \n",
      "  Featurizing 1.mp3 -> .........................DONE\n",
      "  Featurizing 10.mp3 -> .........................DONE\n",
      "  Featurizing 11.mp3 -> .........................DONE\n",
      "  Featurizing 12.mp3 -> .........................DONE\n",
      "  Featurizing 13.mp3 -> .........................DONE\n",
      "  Featurizing 14.mp3 -> .........................DONE\n",
      "  Featurizing 15.mp3 -> .........................DONE\n",
      "  Featurizing 16.mp3 -> .........................DONE\n",
      "  Featurizing 17.mp3 -> .........................DONE\n",
      "  Featurizing 18.mp3 -> .........................DONE\n",
      "  Featurizing 19.mp3 -> .........................DONE\n",
      "  Featurizing 2.mp3 -> .........................DONE\n",
      "  Featurizing 20.mp3 -> .........................DONE\n",
      "  Featurizing 21.mp3 -> .........................DONE\n",
      "  Featurizing 22.mp3 -> .........................DONE\n",
      "  Featurizing 23.mp3 -> .........................DONE\n",
      "  Featurizing 24.mp3 -> .........................DONE\n",
      "  Featurizing 25.mp3 -> .........................DONE\n",
      "  Featurizing 26.mp3 -> .........................DONE\n",
      "  Featurizing 27.mp3 -> .........................DONE\n",
      "  Featurizing 28.mp3 -> .........................DONE\n",
      "  Featurizing 29.mp3 -> .........................DONE\n",
      "  Featurizing 3.mp3 -> .........................DONE\n",
      "  Featurizing 30.mp3 -> .........................DONE\n",
      "  Featurizing 31.mp3 -> .........................DONE\n",
      "  Featurizing 32.mp3 -> .........................DONE\n",
      "  Featurizing 33.mp3 -> .........................DONE\n",
      "  Featurizing 34.mp3 -> .........................DONE\n",
      "  Featurizing 35.mp3 -> .........................DONE\n",
      "  Featurizing 36.mp3 -> .........................DONE\n",
      "  Featurizing 37.mp3 -> .........................DONE\n",
      "  Featurizing 38.mp3 -> .........................DONE\n",
      "  Featurizing 39.mp3 -> .........................DONE\n",
      "  Featurizing 4.mp3 -> .........................DONE\n",
      "  Featurizing 40.mp3 -> .........................DONE\n",
      "  Featurizing 41.mp3 -> .........................DONE\n",
      "  Featurizing 42.mp3 -> .........................DONE\n",
      "  Featurizing 43.mp3 -> .........................DONE\n",
      "  Featurizing 44.mp3 -> .........................DONE\n",
      "  Featurizing 45.mp3 -> .........................DONE\n",
      "  Featurizing 5.mp3 -> .........................DONE\n",
      "  Featurizing 6.mp3 -> .........................DONE\n",
      "  Featurizing 7.mp3 -> .........................DONE\n",
      "  Featurizing 8.mp3 -> .........................DONE\n",
      "  Featurizing 9.mp3 -> .........................DONE\n",
      "[INFO]: Total segments -> 491975\n",
      "[INFO]: Total chords   -> 36\n",
      "[INFO]: Chords labels ['FMin7' 'G7' 'CMin7' 'AbMaj7' 'GbMin7' 'Ab7' 'DbMin7' 'AMaj7' 'GMin7'\n",
      " 'A7' 'DMin7' 'BbMaj7' 'AbMin7' 'Bb7' 'EbMin7' 'BMaj7' 'AMin7' 'B7'\n",
      " 'EMin7' 'CMaj7' 'BbMin7' 'C7' 'DbMaj7' 'BMin7' 'Db7' 'DMaj7' 'D7'\n",
      " 'EbMaj7' 'Eb7' 'EMaj7' 'E7' 'FMaj7' 'F7' 'GbMaj7' 'Gb7' 'GMaj7']\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./process_data_new.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a86b41fd-04c1-483c-b2fc-56926d6c3707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import joblib\n",
    "import random\n",
    "import gc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd50646b-5ad4-4ade-93b2-0b42201c6398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9bdfa92-78f8-4b7c-b19e-8d2b81782799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cens_C</th>\n",
       "      <th>Cens_Db</th>\n",
       "      <th>Cens_D</th>\n",
       "      <th>Cens_Eb</th>\n",
       "      <th>Cens_E</th>\n",
       "      <th>Cens_F</th>\n",
       "      <th>Cens_Gb</th>\n",
       "      <th>Cens_G</th>\n",
       "      <th>Cens_Ab</th>\n",
       "      <th>Cens_A</th>\n",
       "      <th>Cens_Bb</th>\n",
       "      <th>Cens_B</th>\n",
       "      <th>chord</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.458163</td>\n",
       "      <td>0.228498</td>\n",
       "      <td>0.335618</td>\n",
       "      <td>0.385245</td>\n",
       "      <td>0.280831</td>\n",
       "      <td>0.562060</td>\n",
       "      <td>0.101404</td>\n",
       "      <td>0.142517</td>\n",
       "      <td>0.211683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081103</td>\n",
       "      <td>0.007991</td>\n",
       "      <td>FMin7</td>\n",
       "      <td>../Datasets/1.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.456882</td>\n",
       "      <td>0.228481</td>\n",
       "      <td>0.329903</td>\n",
       "      <td>0.386718</td>\n",
       "      <td>0.270461</td>\n",
       "      <td>0.563033</td>\n",
       "      <td>0.104248</td>\n",
       "      <td>0.142325</td>\n",
       "      <td>0.223759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096144</td>\n",
       "      <td>0.008832</td>\n",
       "      <td>FMin7</td>\n",
       "      <td>../Datasets/1.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.454308</td>\n",
       "      <td>0.229081</td>\n",
       "      <td>0.324611</td>\n",
       "      <td>0.388377</td>\n",
       "      <td>0.260669</td>\n",
       "      <td>0.563478</td>\n",
       "      <td>0.107217</td>\n",
       "      <td>0.142526</td>\n",
       "      <td>0.235052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110957</td>\n",
       "      <td>0.009594</td>\n",
       "      <td>FMin7</td>\n",
       "      <td>../Datasets/1.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.450574</td>\n",
       "      <td>0.230340</td>\n",
       "      <td>0.319815</td>\n",
       "      <td>0.390179</td>\n",
       "      <td>0.251621</td>\n",
       "      <td>0.563336</td>\n",
       "      <td>0.110333</td>\n",
       "      <td>0.143008</td>\n",
       "      <td>0.245606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125269</td>\n",
       "      <td>0.010269</td>\n",
       "      <td>FMin7</td>\n",
       "      <td>../Datasets/1.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.445894</td>\n",
       "      <td>0.232287</td>\n",
       "      <td>0.315578</td>\n",
       "      <td>0.392097</td>\n",
       "      <td>0.243367</td>\n",
       "      <td>0.562581</td>\n",
       "      <td>0.113524</td>\n",
       "      <td>0.143594</td>\n",
       "      <td>0.255459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138864</td>\n",
       "      <td>0.010856</td>\n",
       "      <td>FMin7</td>\n",
       "      <td>../Datasets/1.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.207725</td>\n",
       "      <td>0.076254</td>\n",
       "      <td>0.397630</td>\n",
       "      <td>0.012440</td>\n",
       "      <td>0.081124</td>\n",
       "      <td>0.025550</td>\n",
       "      <td>0.113143</td>\n",
       "      <td>0.602074</td>\n",
       "      <td>0.253705</td>\n",
       "      <td>0.415162</td>\n",
       "      <td>0.012520</td>\n",
       "      <td>0.416366</td>\n",
       "      <td>AMin7</td>\n",
       "      <td>../Datasets/9.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.200846</td>\n",
       "      <td>0.072857</td>\n",
       "      <td>0.392787</td>\n",
       "      <td>0.013882</td>\n",
       "      <td>0.080756</td>\n",
       "      <td>0.028333</td>\n",
       "      <td>0.115661</td>\n",
       "      <td>0.609691</td>\n",
       "      <td>0.257939</td>\n",
       "      <td>0.416193</td>\n",
       "      <td>0.013882</td>\n",
       "      <td>0.409283</td>\n",
       "      <td>AMin7</td>\n",
       "      <td>../Datasets/9.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.194574</td>\n",
       "      <td>0.068975</td>\n",
       "      <td>0.387773</td>\n",
       "      <td>0.015407</td>\n",
       "      <td>0.080133</td>\n",
       "      <td>0.031256</td>\n",
       "      <td>0.117944</td>\n",
       "      <td>0.616832</td>\n",
       "      <td>0.261322</td>\n",
       "      <td>0.417602</td>\n",
       "      <td>0.015407</td>\n",
       "      <td>0.402578</td>\n",
       "      <td>AMin7</td>\n",
       "      <td>../Datasets/9.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.188929</td>\n",
       "      <td>0.064633</td>\n",
       "      <td>0.382609</td>\n",
       "      <td>0.017022</td>\n",
       "      <td>0.079286</td>\n",
       "      <td>0.034334</td>\n",
       "      <td>0.119918</td>\n",
       "      <td>0.623479</td>\n",
       "      <td>0.263771</td>\n",
       "      <td>0.419261</td>\n",
       "      <td>0.017022</td>\n",
       "      <td>0.396526</td>\n",
       "      <td>AMin7</td>\n",
       "      <td>../Datasets/9.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.183925</td>\n",
       "      <td>0.059857</td>\n",
       "      <td>0.377302</td>\n",
       "      <td>0.018739</td>\n",
       "      <td>0.078254</td>\n",
       "      <td>0.037583</td>\n",
       "      <td>0.121490</td>\n",
       "      <td>0.629592</td>\n",
       "      <td>0.265280</td>\n",
       "      <td>0.421086</td>\n",
       "      <td>0.018739</td>\n",
       "      <td>0.391356</td>\n",
       "      <td>AMin7</td>\n",
       "      <td>../Datasets/9.mp3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>491975 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Cens_C   Cens_Db    Cens_D   Cens_Eb    Cens_E    Cens_F   Cens_Gb  \\\n",
       "0    0.458163  0.228498  0.335618  0.385245  0.280831  0.562060  0.101404   \n",
       "1    0.456882  0.228481  0.329903  0.386718  0.270461  0.563033  0.104248   \n",
       "2    0.454308  0.229081  0.324611  0.388377  0.260669  0.563478  0.107217   \n",
       "3    0.450574  0.230340  0.319815  0.390179  0.251621  0.563336  0.110333   \n",
       "4    0.445894  0.232287  0.315578  0.392097  0.243367  0.562581  0.113524   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "108  0.207725  0.076254  0.397630  0.012440  0.081124  0.025550  0.113143   \n",
       "109  0.200846  0.072857  0.392787  0.013882  0.080756  0.028333  0.115661   \n",
       "110  0.194574  0.068975  0.387773  0.015407  0.080133  0.031256  0.117944   \n",
       "111  0.188929  0.064633  0.382609  0.017022  0.079286  0.034334  0.119918   \n",
       "112  0.183925  0.059857  0.377302  0.018739  0.078254  0.037583  0.121490   \n",
       "\n",
       "       Cens_G   Cens_Ab    Cens_A   Cens_Bb    Cens_B  chord  \\\n",
       "0    0.142517  0.211683  0.000000  0.081103  0.007991  FMin7   \n",
       "1    0.142325  0.223759  0.000000  0.096144  0.008832  FMin7   \n",
       "2    0.142526  0.235052  0.000000  0.110957  0.009594  FMin7   \n",
       "3    0.143008  0.245606  0.000000  0.125269  0.010269  FMin7   \n",
       "4    0.143594  0.255459  0.000000  0.138864  0.010856  FMin7   \n",
       "..        ...       ...       ...       ...       ...    ...   \n",
       "108  0.602074  0.253705  0.415162  0.012520  0.416366  AMin7   \n",
       "109  0.609691  0.257939  0.416193  0.013882  0.409283  AMin7   \n",
       "110  0.616832  0.261322  0.417602  0.015407  0.402578  AMin7   \n",
       "111  0.623479  0.263771  0.419261  0.017022  0.396526  AMin7   \n",
       "112  0.629592  0.265280  0.421086  0.018739  0.391356  AMin7   \n",
       "\n",
       "              filename  \n",
       "0    ../Datasets/1.mp3  \n",
       "1    ../Datasets/1.mp3  \n",
       "2    ../Datasets/1.mp3  \n",
       "3    ../Datasets/1.mp3  \n",
       "4    ../Datasets/1.mp3  \n",
       "..                 ...  \n",
       "108  ../Datasets/9.mp3  \n",
       "109  ../Datasets/9.mp3  \n",
       "110  ../Datasets/9.mp3  \n",
       "111  ../Datasets/9.mp3  \n",
       "112  ../Datasets/9.mp3  \n",
       "\n",
       "[491975 rows x 14 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_hdf(\"./dataset.h5\", key=\"df\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db232803-2ce8-4648-ad85-b5a16bb05e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_file = df[\"filename\"].unique()\n",
    "train_file, test_file = sk.model_selection.train_test_split(\n",
    "    unique_file,\n",
    "    random_state=42,\n",
    "    test_size=0.2,\n",
    ")\n",
    "test_file, val_file = sk.model_selection.train_test_split(\n",
    "    test_file,\n",
    "    random_state=42,\n",
    "    test_size=0.5,\n",
    ")\n",
    "\n",
    "df_train = df[df[\"filename\"].isin(train_file)]\n",
    "df_val = df[df[\"filename\"].isin(val_file)]\n",
    "df_test = df[df[\"filename\"].isin(test_file)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "155fb292-42c6-4743-bf2d-4325516940ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chords\n",
      "\n",
      "## TRAIN ## chord\n",
      "AbMin7    15671\n",
      "CMin7     15642\n",
      "FMin7     15555\n",
      "GbMin7    15474\n",
      "DbMin7    15298\n",
      "EMin7     14917\n",
      "AMin7     14859\n",
      "BbMin7    14805\n",
      "GMin7     14679\n",
      "EbMin7    14666\n",
      "DMin7     14658\n",
      "BMin7     14601\n",
      "AMaj7     14174\n",
      "AbMaj7    13936\n",
      "EbMaj7    13895\n",
      "EMaj7     13889\n",
      "BbMaj7    13815\n",
      "GbMaj7    13661\n",
      "BMaj7     13602\n",
      "GMaj7     13564\n",
      "DbMaj7    13555\n",
      "DMaj7     13547\n",
      "CMaj7     13523\n",
      "FMaj7     13339\n",
      "G7         3454\n",
      "C7         3395\n",
      "Bb7        3386\n",
      "B7         3360\n",
      "Db7        3232\n",
      "Ab7        3230\n",
      "D7         3200\n",
      "Eb7        3185\n",
      "E7         3175\n",
      "A7         3162\n",
      "Gb7        3155\n",
      "F7         3141\n",
      "Name: count, dtype: int64\n",
      "\n",
      "## VAL ## chord\n",
      "DMaj7     4538\n",
      "GMaj7     3987\n",
      "FMaj7     3514\n",
      "DbMaj7    3511\n",
      "EbMaj7    3497\n",
      "BbMaj7    3489\n",
      "GbMaj7    3489\n",
      "BMaj7     3489\n",
      "CMaj7     3489\n",
      "AMaj7     3474\n",
      "EMaj7     3474\n",
      "AbMaj7    3474\n",
      "EMin7     2223\n",
      "AMin7     1804\n",
      "DMin7     1774\n",
      "BbMin7    1758\n",
      "GMin7     1756\n",
      "CMin7     1753\n",
      "GbMin7    1746\n",
      "AbMin7    1741\n",
      "EbMin7    1732\n",
      "FMin7     1731\n",
      "DbMin7    1716\n",
      "BMin7     1716\n",
      "Bb7        178\n",
      "A7         167\n",
      "D7         145\n",
      "Eb7        140\n",
      "Gb7        140\n",
      "F7         140\n",
      "E7         140\n",
      "G7         140\n",
      "Ab7        140\n",
      "B7         140\n",
      "C7         140\n",
      "Db7        140\n",
      "Name: count, dtype: int64\n",
      "\n",
      "## TEST ## chord\n",
      "GMaj7     2189\n",
      "CMaj7     2034\n",
      "AMaj7     1986\n",
      "GbMaj7    1975\n",
      "DMaj7     1969\n",
      "BbMaj7    1938\n",
      "FMaj7     1886\n",
      "EMaj7     1884\n",
      "EbMaj7    1866\n",
      "AbMaj7    1866\n",
      "BMaj7     1866\n",
      "DbMaj7    1866\n",
      "BMin7     1512\n",
      "EMin7     1375\n",
      "DMin7     1287\n",
      "AbMin7    1281\n",
      "BbMin7    1274\n",
      "DbMin7    1273\n",
      "EbMin7    1253\n",
      "FMin7     1234\n",
      "GbMin7    1234\n",
      "CMin7     1234\n",
      "AMin7     1234\n",
      "GMin7     1234\n",
      "G7         229\n",
      "Bb7        211\n",
      "A7         176\n",
      "Ab7        176\n",
      "Db7        176\n",
      "C7         176\n",
      "B7         176\n",
      "D7         176\n",
      "Eb7        176\n",
      "E7         176\n",
      "F7         176\n",
      "Gb7        176\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Chords\")\n",
    "print(\"\\n## TRAIN ##\", df_train[\"chord\"].value_counts())\n",
    "print(\"\\n## VAL ##\", df_val[\"chord\"].value_counts())\n",
    "print(\"\\n## TEST ##\", df_test[\"chord\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f22910a2-a2b3-4672-8409-0d9335166d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./encoder.xz']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y = df[\"chord\"]\n",
    "# X = df.drop(columns=\"chord\")\n",
    "# encoder = sk.preprocessing.LabelEncoder()\n",
    "# y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "# joblib.dump(encoder, \"./encoder.xz\")\n",
    "\n",
    "def get_Xy(df: pd.DataFrame):\n",
    "    return df.drop([\"chord\", \"filename\"], axis=1), df[\"chord\"]\n",
    "\n",
    "X_train, y_train = get_Xy(df_train)\n",
    "X_val, y_val = get_Xy(df_val)\n",
    "X_test, y_test = get_Xy(df_test)\n",
    "\n",
    "encoder = sk.preprocessing.LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "y_val_encoded = encoder.transform(y_val)\n",
    "y_test_encoded = encoder.transform(y_test)\n",
    "\n",
    "joblib.dump(encoder, \"./encoder.xz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01f9d408-e165-4e68-941f-ccdcd4c36b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del df\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb64354-b100-49f1-bce9-dc17f43e08b2",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a22ecbda-f306-4fea-9746-7d4ca17726fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEQUENCE_LEN = 20 # 0.1 * 20.0 = 2 sec of sequence data\n",
    "\n",
    "# def make_seq(X, y_encoded):\n",
    "#     X_seq, y_encoded_seq = None, None\n",
    "#     X_seq_list = []\n",
    "#     y_encoded_seq_list = []\n",
    "#     for i in range(len(X) - SEQUENCE_LEN + 1):\n",
    "#         X_seq_list.append(X.values[i : i + SEQUENCE_LEN, :])\n",
    "#         y_encoded_seq_list.append(y_encoded[i + SEQUENCE_LEN - 1])\n",
    "    \n",
    "#     return np.array(X_seq_list), np.array(y_encoded_seq_list)\n",
    "\n",
    "# X_train, y_train_encoded = make_seq(X_train, y_train_encoded)\n",
    "# X_val, y_val_encoded = make_seq(X_val, y_val_encoded)\n",
    "# X_test, y_test_encoded = make_seq(X_test, y_test_encoded)\n",
    "\n",
    "# print(\"TRAIN\")\n",
    "# print(\"X sequence shape: \", X_train.shape)\n",
    "# print(\"y sequence shape: \", y_train_encoded.shape)\n",
    "\n",
    "# print(\"\\nVAL\")\n",
    "# print(\"X sequence shape: \", X_val.shape)\n",
    "# print(\"y sequence shape: \", y_val_encoded.shape)\n",
    "\n",
    "# print(\"\\nTEST\")\n",
    "# print(\"X sequence shape: \", X_test.shape)\n",
    "# print(\"y sequence shape: \", y_test_encoded.shape)\n",
    "\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b2ec4ba-ec19-4210-b01d-bbc116eac1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_seq_train, X_seq_test, y_seq_train, y_seq_test = sk.model_selection.train_test_split(\n",
    "#     X_seq,\n",
    "#     y_encoded_seq,\n",
    "#     test_size=0.2,\n",
    "#     random_state=42,\n",
    "#     stratify=y_encoded_seq,\n",
    "# )\n",
    "\n",
    "# print(f\"X_train: {len(X_seq_train)}\")\n",
    "# print(f\"y_train: {len(y_seq_train)}\")\n",
    "# print(f\"X_test:  {len(X_seq_test)}\")\n",
    "# print(f\"y_test:  {len(y_seq_test)}\")\n",
    "\n",
    "# del X_seq\n",
    "# del y_encoded_seq\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57f6e917-5e8b-49c4-bb6f-97258dfa85e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Total feature: \", X_train.shape[2])\n",
    "# print(\"Total class:   \", len(encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bce6e562-b4ae-4bde-bc29-4b1f70ce3550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = sk.utils.class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train,\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "adb2f230-0a9d-455c-86f4-4fb5ac20590a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 5ms/step - accuracy: 0.4031 - loss: 2.2809 - val_accuracy: 0.6739 - val_loss: 1.2393\n",
      "Epoch 2/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 5ms/step - accuracy: 0.5109 - loss: 1.7479 - val_accuracy: 0.6751 - val_loss: 1.2096\n",
      "Epoch 3/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 5ms/step - accuracy: 0.5185 - loss: 1.7125 - val_accuracy: 0.6625 - val_loss: 1.2186\n",
      "Epoch 4/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 5ms/step - accuracy: 0.5194 - loss: 1.7088 - val_accuracy: 0.6580 - val_loss: 1.2083\n",
      "Epoch 5/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 5ms/step - accuracy: 0.5259 - loss: 1.6773 - val_accuracy: 0.6581 - val_loss: 1.2255\n",
      "Epoch 6/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 5ms/step - accuracy: 0.5278 - loss: 1.6723 - val_accuracy: 0.6536 - val_loss: 1.2323\n",
      "Epoch 7/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 5ms/step - accuracy: 0.5295 - loss: 1.6664 - val_accuracy: 0.6590 - val_loss: 1.2154\n",
      "Epoch 8/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 5ms/step - accuracy: 0.5324 - loss: 1.6512 - val_accuracy: 0.6525 - val_loss: 1.2228\n",
      "Epoch 9/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 5ms/step - accuracy: 0.5298 - loss: 1.6608 - val_accuracy: 0.6458 - val_loss: 1.2444\n",
      "Epoch 10/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 5ms/step - accuracy: 0.5307 - loss: 1.6527 - val_accuracy: 0.6632 - val_loss: 1.1991\n",
      "Epoch 11/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 5ms/step - accuracy: 0.5321 - loss: 1.6459 - val_accuracy: 0.6572 - val_loss: 1.2191\n",
      "Epoch 12/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 5ms/step - accuracy: 0.5318 - loss: 1.6484 - val_accuracy: 0.6539 - val_loss: 1.2092\n",
      "Epoch 13/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 5ms/step - accuracy: 0.5327 - loss: 1.6485 - val_accuracy: 0.6559 - val_loss: 1.2208\n",
      "Epoch 14/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 5ms/step - accuracy: 0.5306 - loss: 1.6523 - val_accuracy: 0.6496 - val_loss: 1.2228\n",
      "Epoch 15/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 5ms/step - accuracy: 0.5331 - loss: 1.6378 - val_accuracy: 0.6542 - val_loss: 1.2299\n",
      "Epoch 16/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 5ms/step - accuracy: 0.5364 - loss: 1.6304 - val_accuracy: 0.6466 - val_loss: 1.2403\n",
      "Epoch 17/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 5ms/step - accuracy: 0.5337 - loss: 1.6346 - val_accuracy: 0.6435 - val_loss: 1.2438\n",
      "Epoch 18/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 5ms/step - accuracy: 0.5337 - loss: 1.6326 - val_accuracy: 0.6478 - val_loss: 1.2219\n",
      "Epoch 19/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 5ms/step - accuracy: 0.5339 - loss: 1.6327 - val_accuracy: 0.6594 - val_loss: 1.2138\n",
      "Epoch 20/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 5ms/step - accuracy: 0.5345 - loss: 1.6362 - val_accuracy: 0.6445 - val_loss: 1.2384\n",
      "::::: model_lstm_16_16_seq_5 :::::\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.5956 - loss: 1.4165\n",
      "[1.6271724700927734, 0.5599570274353027]\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A7       0.25      0.86      0.39       176\n",
      "       AMaj7       0.73      0.59      0.65      1986\n",
      "       AMin7       0.51      0.42      0.46      1234\n",
      "         Ab7       0.29      0.94      0.44       176\n",
      "      AbMaj7       0.72      0.58      0.64      1866\n",
      "      AbMin7       0.58      0.54      0.56      1281\n",
      "          B7       0.17      0.97      0.29       176\n",
      "       BMaj7       0.67      0.59      0.63      1866\n",
      "       BMin7       0.60      0.41      0.49      1512\n",
      "         Bb7       0.28      0.90      0.42       211\n",
      "      BbMaj7       0.71      0.55      0.62      1938\n",
      "      BbMin7       0.62      0.46      0.53      1274\n",
      "          C7       0.25      0.94      0.39       176\n",
      "       CMaj7       0.69      0.56      0.62      2034\n",
      "       CMin7       0.61      0.49      0.54      1234\n",
      "          D7       0.20      0.94      0.33       176\n",
      "       DMaj7       0.69      0.54      0.61      1969\n",
      "       DMin7       0.54      0.50      0.52      1283\n",
      "         Db7       0.24      0.90      0.37       176\n",
      "      DbMaj7       0.65      0.59      0.62      1866\n",
      "      DbMin7       0.58      0.41      0.48      1273\n",
      "          E7       0.20      0.95      0.33       176\n",
      "       EMaj7       0.63      0.59      0.61      1884\n",
      "       EMin7       0.62      0.48      0.54      1375\n",
      "         Eb7       0.27      0.97      0.43       176\n",
      "      EbMaj7       0.74      0.56      0.64      1866\n",
      "      EbMin7       0.59      0.56      0.58      1253\n",
      "          F7       0.20      0.98      0.33       176\n",
      "       FMaj7       0.61      0.62      0.61      1886\n",
      "       FMin7       0.56      0.43      0.49      1234\n",
      "          G7       0.32      0.93      0.47       229\n",
      "       GMaj7       0.73      0.60      0.66      2189\n",
      "       GMin7       0.60      0.51      0.55      1234\n",
      "         Gb7       0.26      0.95      0.41       176\n",
      "      GbMaj7       0.74      0.57      0.65      1975\n",
      "      GbMin7       0.57      0.52      0.54      1234\n",
      "\n",
      "    accuracy                           0.56     40946\n",
      "   macro avg       0.51      0.66      0.51     40946\n",
      "weighted avg       0.63      0.56      0.58     40946\n",
      "\n",
      "\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 6ms/step - accuracy: 0.4642 - loss: 2.0770 - val_accuracy: 0.6703 - val_loss: 1.2368\n",
      "Epoch 2/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 6ms/step - accuracy: 0.5444 - loss: 1.6371 - val_accuracy: 0.6648 - val_loss: 1.2156\n",
      "Epoch 3/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 6ms/step - accuracy: 0.5522 - loss: 1.5967 - val_accuracy: 0.6893 - val_loss: 1.1834\n",
      "Epoch 4/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 6ms/step - accuracy: 0.5583 - loss: 1.5713 - val_accuracy: 0.6651 - val_loss: 1.2305\n",
      "Epoch 5/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 6ms/step - accuracy: 0.5608 - loss: 1.5480 - val_accuracy: 0.6694 - val_loss: 1.2036\n",
      "Epoch 6/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 6ms/step - accuracy: 0.5647 - loss: 1.5332 - val_accuracy: 0.6659 - val_loss: 1.2212\n",
      "Epoch 7/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 6ms/step - accuracy: 0.5661 - loss: 1.5323 - val_accuracy: 0.6777 - val_loss: 1.2052\n",
      "Epoch 8/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 6ms/step - accuracy: 0.5677 - loss: 1.5206 - val_accuracy: 0.6696 - val_loss: 1.2159\n",
      "Epoch 9/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 6ms/step - accuracy: 0.5668 - loss: 1.5151 - val_accuracy: 0.6717 - val_loss: 1.2204\n",
      "Epoch 10/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 6ms/step - accuracy: 0.5686 - loss: 1.5169 - val_accuracy: 0.6728 - val_loss: 1.1913\n",
      "Epoch 11/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 6ms/step - accuracy: 0.5713 - loss: 1.5068 - val_accuracy: 0.6766 - val_loss: 1.2034\n",
      "Epoch 12/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 6ms/step - accuracy: 0.5674 - loss: 1.5110 - val_accuracy: 0.6593 - val_loss: 1.2334\n",
      "Epoch 13/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 6ms/step - accuracy: 0.5714 - loss: 1.5013 - val_accuracy: 0.6747 - val_loss: 1.2012\n",
      "::::: model_lstm_32_32_seq_5 :::::\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.6011 - loss: 1.4191\n",
      "[1.6505727767944336, 0.5644263029098511]\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A7       0.24      0.94      0.38       176\n",
      "       AMaj7       0.73      0.54      0.62      1986\n",
      "       AMin7       0.60      0.38      0.47      1234\n",
      "         Ab7       0.22      0.90      0.35       176\n",
      "      AbMaj7       0.74      0.53      0.62      1866\n",
      "      AbMin7       0.60      0.59      0.59      1281\n",
      "          B7       0.30      0.94      0.46       176\n",
      "       BMaj7       0.64      0.62      0.63      1866\n",
      "       BMin7       0.62      0.46      0.53      1512\n",
      "         Bb7       0.40      0.95      0.56       211\n",
      "      BbMaj7       0.72      0.54      0.62      1938\n",
      "      BbMin7       0.59      0.51      0.54      1274\n",
      "          C7       0.23      0.94      0.37       176\n",
      "       CMaj7       0.65      0.60      0.62      2034\n",
      "       CMin7       0.62      0.57      0.59      1234\n",
      "          D7       0.19      0.97      0.31       176\n",
      "       DMaj7       0.66      0.54      0.59      1969\n",
      "       DMin7       0.54      0.45      0.49      1283\n",
      "         Db7       0.18      0.95      0.31       176\n",
      "      DbMaj7       0.65      0.58      0.61      1866\n",
      "      DbMin7       0.50      0.47      0.48      1273\n",
      "          E7       0.36      0.88      0.51       176\n",
      "       EMaj7       0.66      0.58      0.62      1884\n",
      "       EMin7       0.61      0.55      0.58      1375\n",
      "         Eb7       0.25      0.95      0.39       176\n",
      "      EbMaj7       0.78      0.52      0.62      1866\n",
      "      EbMin7       0.59      0.55      0.57      1253\n",
      "          F7       0.26      0.99      0.42       176\n",
      "       FMaj7       0.65      0.58      0.61      1886\n",
      "       FMin7       0.56      0.49      0.52      1234\n",
      "          G7       0.27      0.96      0.42       229\n",
      "       GMaj7       0.76      0.55      0.64      2189\n",
      "       GMin7       0.58      0.49      0.53      1234\n",
      "         Gb7       0.26      0.99      0.41       176\n",
      "      GbMaj7       0.68      0.61      0.64      1975\n",
      "      GbMin7       0.56      0.60      0.58      1234\n",
      "\n",
      "    accuracy                           0.56     40946\n",
      "   macro avg       0.51      0.67      0.52     40946\n",
      "weighted avg       0.63      0.56      0.58     40946\n",
      "\n",
      "\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 7ms/step - accuracy: 0.4952 - loss: 1.9823 - val_accuracy: 0.6652 - val_loss: 1.2697\n",
      "Epoch 2/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 7ms/step - accuracy: 0.5641 - loss: 1.5921 - val_accuracy: 0.6532 - val_loss: 1.2820\n",
      "Epoch 3/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 7ms/step - accuracy: 0.5737 - loss: 1.5418 - val_accuracy: 0.6605 - val_loss: 1.2578\n",
      "Epoch 4/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 7ms/step - accuracy: 0.5815 - loss: 1.5081 - val_accuracy: 0.6768 - val_loss: 1.2505\n",
      "Epoch 5/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 7ms/step - accuracy: 0.5843 - loss: 1.4885 - val_accuracy: 0.6726 - val_loss: 1.2598\n",
      "Epoch 6/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 7ms/step - accuracy: 0.5859 - loss: 1.4720 - val_accuracy: 0.6866 - val_loss: 1.2245\n",
      "Epoch 7/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 7ms/step - accuracy: 0.5909 - loss: 1.4589 - val_accuracy: 0.6616 - val_loss: 1.2685\n",
      "Epoch 8/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 7ms/step - accuracy: 0.5928 - loss: 1.4501 - val_accuracy: 0.6881 - val_loss: 1.2183\n",
      "Epoch 9/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 7ms/step - accuracy: 0.5941 - loss: 1.4433 - val_accuracy: 0.6909 - val_loss: 1.2088\n",
      "Epoch 10/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 7ms/step - accuracy: 0.5960 - loss: 1.4363 - val_accuracy: 0.6570 - val_loss: 1.2799\n",
      "Epoch 11/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 7ms/step - accuracy: 0.5934 - loss: 1.4362 - val_accuracy: 0.6839 - val_loss: 1.2346\n",
      "Epoch 12/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 7ms/step - accuracy: 0.5962 - loss: 1.4348 - val_accuracy: 0.6683 - val_loss: 1.2761\n",
      "Epoch 13/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 7ms/step - accuracy: 0.5963 - loss: 1.4309 - val_accuracy: 0.6809 - val_loss: 1.2479\n",
      "Epoch 14/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 7ms/step - accuracy: 0.5976 - loss: 1.4262 - val_accuracy: 0.6760 - val_loss: 1.2431\n",
      "Epoch 15/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 7ms/step - accuracy: 0.5969 - loss: 1.4235 - val_accuracy: 0.6726 - val_loss: 1.2631\n",
      "Epoch 16/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 7ms/step - accuracy: 0.5995 - loss: 1.4217 - val_accuracy: 0.6846 - val_loss: 1.2264\n",
      "Epoch 17/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 7ms/step - accuracy: 0.5997 - loss: 1.4158 - val_accuracy: 0.6763 - val_loss: 1.2476\n",
      "Epoch 18/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 7ms/step - accuracy: 0.6009 - loss: 1.4130 - val_accuracy: 0.6790 - val_loss: 1.2395\n",
      "Epoch 19/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 7ms/step - accuracy: 0.5991 - loss: 1.4213 - val_accuracy: 0.6691 - val_loss: 1.2538\n",
      "::::: model_lstm_64_64_seq_5 :::::\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.6048 - loss: 1.4418\n",
      "[1.6794637441635132, 0.5672348737716675]\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A7       0.24      0.96      0.38       176\n",
      "       AMaj7       0.72      0.53      0.61      1986\n",
      "       AMin7       0.51      0.45      0.48      1234\n",
      "         Ab7       0.25      0.91      0.39       176\n",
      "      AbMaj7       0.65      0.62      0.64      1866\n",
      "      AbMin7       0.59      0.46      0.52      1281\n",
      "          B7       0.32      0.95      0.48       176\n",
      "       BMaj7       0.67      0.56      0.61      1866\n",
      "       BMin7       0.59      0.57      0.58      1512\n",
      "         Bb7       0.28      0.89      0.43       211\n",
      "      BbMaj7       0.73      0.56      0.63      1938\n",
      "      BbMin7       0.59      0.49      0.53      1274\n",
      "          C7       0.50      0.93      0.65       176\n",
      "       CMaj7       0.63      0.61      0.62      2034\n",
      "       CMin7       0.61      0.51      0.56      1234\n",
      "          D7       0.28      0.94      0.43       176\n",
      "       DMaj7       0.69      0.57      0.62      1969\n",
      "       DMin7       0.55      0.52      0.53      1283\n",
      "         Db7       0.25      0.94      0.40       176\n",
      "      DbMaj7       0.67      0.58      0.62      1866\n",
      "      DbMin7       0.50      0.47      0.49      1273\n",
      "          E7       0.35      0.86      0.50       176\n",
      "       EMaj7       0.65      0.60      0.63      1884\n",
      "       EMin7       0.58      0.50      0.54      1375\n",
      "         Eb7       0.37      0.89      0.52       176\n",
      "      EbMaj7       0.71      0.59      0.65      1866\n",
      "      EbMin7       0.53      0.39      0.45      1253\n",
      "          F7       0.22      0.97      0.36       176\n",
      "       FMaj7       0.63      0.59      0.61      1886\n",
      "       FMin7       0.54      0.45      0.49      1234\n",
      "          G7       0.19      0.90      0.32       229\n",
      "       GMaj7       0.71      0.59      0.64      2189\n",
      "       GMin7       0.59      0.47      0.52      1234\n",
      "         Gb7       0.31      0.85      0.46       176\n",
      "      GbMaj7       0.71      0.62      0.66      1975\n",
      "      GbMin7       0.54      0.58      0.56      1234\n",
      "\n",
      "    accuracy                           0.57     40946\n",
      "   macro avg       0.51      0.66      0.53     40946\n",
      "weighted avg       0.61      0.57      0.58     40946\n",
      "\n",
      "\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 9ms/step - accuracy: 0.3969 - loss: 2.3031 - val_accuracy: 0.6241 - val_loss: 1.3265\n",
      "Epoch 2/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 9ms/step - accuracy: 0.5099 - loss: 1.7473 - val_accuracy: 0.6517 - val_loss: 1.2405\n",
      "Epoch 3/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 9ms/step - accuracy: 0.5180 - loss: 1.7034 - val_accuracy: 0.6498 - val_loss: 1.2431\n",
      "Epoch 4/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 9ms/step - accuracy: 0.5222 - loss: 1.6807 - val_accuracy: 0.6431 - val_loss: 1.2519\n",
      "Epoch 5/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 9ms/step - accuracy: 0.5275 - loss: 1.6674 - val_accuracy: 0.6322 - val_loss: 1.2622\n",
      "Epoch 6/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 9ms/step - accuracy: 0.5299 - loss: 1.6599 - val_accuracy: 0.6375 - val_loss: 1.2588\n",
      "Epoch 7/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 9ms/step - accuracy: 0.5290 - loss: 1.6511 - val_accuracy: 0.6454 - val_loss: 1.2422\n",
      "Epoch 8/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 9ms/step - accuracy: 0.5313 - loss: 1.6480 - val_accuracy: 0.6542 - val_loss: 1.2171\n",
      "Epoch 9/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 9ms/step - accuracy: 0.5339 - loss: 1.6372 - val_accuracy: 0.6342 - val_loss: 1.2553\n",
      "Epoch 10/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 9ms/step - accuracy: 0.5323 - loss: 1.6364 - val_accuracy: 0.6387 - val_loss: 1.2482\n",
      "Epoch 11/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 9ms/step - accuracy: 0.5328 - loss: 1.6333 - val_accuracy: 0.6379 - val_loss: 1.2454\n",
      "Epoch 12/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 9ms/step - accuracy: 0.5332 - loss: 1.6369 - val_accuracy: 0.6450 - val_loss: 1.2279\n",
      "Epoch 13/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 9ms/step - accuracy: 0.5347 - loss: 1.6234 - val_accuracy: 0.6188 - val_loss: 1.2677\n",
      "Epoch 14/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 9ms/step - accuracy: 0.5331 - loss: 1.6264 - val_accuracy: 0.6330 - val_loss: 1.2710\n",
      "Epoch 15/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 9ms/step - accuracy: 0.5365 - loss: 1.6230 - val_accuracy: 0.6428 - val_loss: 1.2387\n",
      "Epoch 16/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 9ms/step - accuracy: 0.5359 - loss: 1.6207 - val_accuracy: 0.6381 - val_loss: 1.2666\n",
      "Epoch 17/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 9ms/step - accuracy: 0.5340 - loss: 1.6221 - val_accuracy: 0.6389 - val_loss: 1.2344\n",
      "Epoch 18/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 9ms/step - accuracy: 0.5354 - loss: 1.6256 - val_accuracy: 0.6350 - val_loss: 1.2415\n",
      "::::: model_lstm_16_16_seq_10 :::::\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5908 - loss: 1.4292\n",
      "[1.6742970943450928, 0.5433672666549683]\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A7       0.17      0.90      0.29       176\n",
      "       AMaj7       0.71      0.53      0.61      1986\n",
      "       AMin7       0.51      0.35      0.41      1234\n",
      "         Ab7       0.28      0.97      0.43       176\n",
      "      AbMaj7       0.70      0.52      0.60      1866\n",
      "      AbMin7       0.57      0.44      0.50      1281\n",
      "          B7       0.24      0.97      0.38       176\n",
      "       BMaj7       0.64      0.55      0.59      1866\n",
      "       BMin7       0.58      0.54      0.56      1512\n",
      "         Bb7       0.26      0.93      0.41       211\n",
      "      BbMaj7       0.69      0.52      0.60      1938\n",
      "      BbMin7       0.60      0.46      0.52      1274\n",
      "          C7       0.23      0.96      0.37       176\n",
      "       CMaj7       0.60      0.62      0.61      2034\n",
      "       CMin7       0.60      0.51      0.55      1234\n",
      "          D7       0.29      0.95      0.44       176\n",
      "       DMaj7       0.73      0.50      0.59      1969\n",
      "       DMin7       0.57      0.52      0.54      1278\n",
      "         Db7       0.18      0.97      0.31       176\n",
      "      DbMaj7       0.59      0.59      0.59      1866\n",
      "      DbMin7       0.45      0.38      0.42      1273\n",
      "          E7       0.25      0.94      0.39       176\n",
      "       EMaj7       0.70      0.55      0.62      1884\n",
      "       EMin7       0.59      0.54      0.56      1375\n",
      "         Eb7       0.25      0.97      0.39       176\n",
      "      EbMaj7       0.68      0.56      0.62      1866\n",
      "      EbMin7       0.57      0.48      0.52      1253\n",
      "          F7       0.26      0.95      0.40       176\n",
      "       FMaj7       0.71      0.53      0.61      1886\n",
      "       FMin7       0.61      0.46      0.52      1234\n",
      "          G7       0.23      0.96      0.38       229\n",
      "       GMaj7       0.71      0.55      0.62      2189\n",
      "       GMin7       0.53      0.51      0.52      1234\n",
      "         Gb7       0.22      0.97      0.36       176\n",
      "      GbMaj7       0.69      0.57      0.63      1975\n",
      "      GbMin7       0.59      0.51      0.55      1234\n",
      "\n",
      "    accuracy                           0.54     40941\n",
      "   macro avg       0.49      0.66      0.50     40941\n",
      "weighted avg       0.61      0.54      0.56     40941\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 9ms/step - accuracy: 0.4519 - loss: 2.1002 - val_accuracy: 0.6546 - val_loss: 1.2518\n",
      "Epoch 2/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 10ms/step - accuracy: 0.5449 - loss: 1.6251 - val_accuracy: 0.6476 - val_loss: 1.2747\n",
      "Epoch 3/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 10ms/step - accuracy: 0.5548 - loss: 1.5784 - val_accuracy: 0.6608 - val_loss: 1.2390\n",
      "Epoch 4/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 10ms/step - accuracy: 0.5617 - loss: 1.5560 - val_accuracy: 0.6600 - val_loss: 1.2312\n",
      "Epoch 5/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 10ms/step - accuracy: 0.5631 - loss: 1.5357 - val_accuracy: 0.6528 - val_loss: 1.2661\n",
      "Epoch 6/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 10ms/step - accuracy: 0.5694 - loss: 1.5198 - val_accuracy: 0.6646 - val_loss: 1.2284\n",
      "Epoch 7/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 10ms/step - accuracy: 0.5690 - loss: 1.5178 - val_accuracy: 0.6750 - val_loss: 1.2241\n",
      "Epoch 8/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 10ms/step - accuracy: 0.5716 - loss: 1.5097 - val_accuracy: 0.6696 - val_loss: 1.2271\n",
      "Epoch 9/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 10ms/step - accuracy: 0.5721 - loss: 1.4996 - val_accuracy: 0.6559 - val_loss: 1.2489\n",
      "Epoch 10/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 10ms/step - accuracy: 0.5718 - loss: 1.5033 - val_accuracy: 0.6642 - val_loss: 1.2436\n",
      "Epoch 11/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 10ms/step - accuracy: 0.5732 - loss: 1.4973 - val_accuracy: 0.6720 - val_loss: 1.2305\n",
      "Epoch 12/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 10ms/step - accuracy: 0.5764 - loss: 1.4883 - val_accuracy: 0.6305 - val_loss: 1.2978\n",
      "Epoch 13/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 10ms/step - accuracy: 0.5740 - loss: 1.4897 - val_accuracy: 0.6678 - val_loss: 1.2300\n",
      "Epoch 14/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 10ms/step - accuracy: 0.5749 - loss: 1.4886 - val_accuracy: 0.6713 - val_loss: 1.2263\n",
      "Epoch 15/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 10ms/step - accuracy: 0.5760 - loss: 1.4793 - val_accuracy: 0.6599 - val_loss: 1.2472\n",
      "Epoch 16/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 10ms/step - accuracy: 0.5742 - loss: 1.4859 - val_accuracy: 0.6706 - val_loss: 1.2320\n",
      "Epoch 17/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 10ms/step - accuracy: 0.5752 - loss: 1.4890 - val_accuracy: 0.6639 - val_loss: 1.2487\n",
      "::::: model_lstm_32_32_seq_10 :::::\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.5914 - loss: 1.4739\n",
      "[1.7043836116790771, 0.5521115660667419]\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A7       0.21      0.84      0.33       176\n",
      "       AMaj7       0.71      0.54      0.61      1986\n",
      "       AMin7       0.52      0.43      0.47      1234\n",
      "         Ab7       0.22      0.90      0.36       176\n",
      "      AbMaj7       0.70      0.56      0.63      1866\n",
      "      AbMin7       0.60      0.47      0.52      1281\n",
      "          B7       0.28      0.93      0.44       176\n",
      "       BMaj7       0.67      0.58      0.62      1866\n",
      "       BMin7       0.60      0.53      0.57      1512\n",
      "         Bb7       0.38      0.91      0.54       211\n",
      "      BbMaj7       0.69      0.51      0.59      1938\n",
      "      BbMin7       0.59      0.46      0.52      1274\n",
      "          C7       0.20      0.93      0.33       176\n",
      "       CMaj7       0.58      0.62      0.60      2034\n",
      "       CMin7       0.51      0.54      0.53      1234\n",
      "          D7       0.15      0.98      0.27       176\n",
      "       DMaj7       0.70      0.57      0.63      1969\n",
      "       DMin7       0.58      0.45      0.51      1278\n",
      "         Db7       0.31      0.90      0.46       176\n",
      "      DbMaj7       0.71      0.51      0.59      1866\n",
      "      DbMin7       0.51      0.51      0.51      1273\n",
      "          E7       0.36      0.90      0.51       176\n",
      "       EMaj7       0.66      0.56      0.61      1884\n",
      "       EMin7       0.62      0.51      0.56      1375\n",
      "         Eb7       0.29      0.90      0.44       176\n",
      "      EbMaj7       0.73      0.54      0.62      1866\n",
      "      EbMin7       0.57      0.57      0.57      1253\n",
      "          F7       0.19      0.98      0.31       176\n",
      "       FMaj7       0.64      0.59      0.62      1886\n",
      "       FMin7       0.60      0.39      0.48      1234\n",
      "          G7       0.31      0.95      0.47       229\n",
      "       GMaj7       0.75      0.53      0.62      2189\n",
      "       GMin7       0.55      0.51      0.53      1234\n",
      "         Gb7       0.19      0.93      0.32       176\n",
      "      GbMaj7       0.75      0.51      0.61      1975\n",
      "      GbMin7       0.50      0.61      0.55      1234\n",
      "\n",
      "    accuracy                           0.55     40941\n",
      "   macro avg       0.50      0.66      0.51     40941\n",
      "weighted avg       0.62      0.55      0.57     40941\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 11ms/step - accuracy: 0.4822 - loss: 2.0063 - val_accuracy: 0.6377 - val_loss: 1.3131\n",
      "Epoch 2/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 12ms/step - accuracy: 0.5612 - loss: 1.5928 - val_accuracy: 0.6652 - val_loss: 1.2881\n",
      "Epoch 3/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 12ms/step - accuracy: 0.5762 - loss: 1.5338 - val_accuracy: 0.6680 - val_loss: 1.2654\n",
      "Epoch 4/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 12ms/step - accuracy: 0.5837 - loss: 1.5018 - val_accuracy: 0.6579 - val_loss: 1.3019\n",
      "Epoch 5/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 12ms/step - accuracy: 0.5876 - loss: 1.4777 - val_accuracy: 0.6453 - val_loss: 1.3304\n",
      "Epoch 6/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 12ms/step - accuracy: 0.5934 - loss: 1.4614 - val_accuracy: 0.6614 - val_loss: 1.2859\n",
      "Epoch 7/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 12ms/step - accuracy: 0.5960 - loss: 1.4539 - val_accuracy: 0.6675 - val_loss: 1.2821\n",
      "Epoch 8/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 12ms/step - accuracy: 0.5954 - loss: 1.4527 - val_accuracy: 0.6481 - val_loss: 1.3058\n",
      "Epoch 9/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 12ms/step - accuracy: 0.5981 - loss: 1.4418 - val_accuracy: 0.6535 - val_loss: 1.3154\n",
      "Epoch 10/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 12ms/step - accuracy: 0.5983 - loss: 1.4387 - val_accuracy: 0.6506 - val_loss: 1.3338\n",
      "Epoch 11/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 12ms/step - accuracy: 0.5981 - loss: 1.4372 - val_accuracy: 0.6553 - val_loss: 1.3172\n",
      "Epoch 12/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 12ms/step - accuracy: 0.5992 - loss: 1.4333 - val_accuracy: 0.6799 - val_loss: 1.2500\n",
      "Epoch 13/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 12ms/step - accuracy: 0.5996 - loss: 1.4310 - val_accuracy: 0.6756 - val_loss: 1.2669\n",
      "Epoch 14/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 12ms/step - accuracy: 0.6004 - loss: 1.4259 - val_accuracy: 0.6697 - val_loss: 1.2914\n",
      "Epoch 15/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 12ms/step - accuracy: 0.6006 - loss: 1.4238 - val_accuracy: 0.6770 - val_loss: 1.2599\n",
      "Epoch 16/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 12ms/step - accuracy: 0.6031 - loss: 1.4220 - val_accuracy: 0.6780 - val_loss: 1.2521\n",
      "Epoch 17/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 12ms/step - accuracy: 0.6022 - loss: 1.4223 - val_accuracy: 0.6470 - val_loss: 1.3357\n",
      "Epoch 18/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 12ms/step - accuracy: 0.6014 - loss: 1.4162 - val_accuracy: 0.6669 - val_loss: 1.2837\n",
      "Epoch 19/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 12ms/step - accuracy: 0.6034 - loss: 1.4135 - val_accuracy: 0.6394 - val_loss: 1.3539\n",
      "Epoch 20/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 12ms/step - accuracy: 0.6025 - loss: 1.4183 - val_accuracy: 0.6599 - val_loss: 1.2841\n",
      "Epoch 21/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 12ms/step - accuracy: 0.6029 - loss: 1.4142 - val_accuracy: 0.6833 - val_loss: 1.2504\n",
      "Epoch 22/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 13ms/step - accuracy: 0.6036 - loss: 1.4138 - val_accuracy: 0.6744 - val_loss: 1.2637\n",
      "::::: model_lstm_64_64_seq_10 :::::\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.5930 - loss: 1.4841\n",
      "[1.7381004095077515, 0.54920494556427]\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A7       0.24      0.94      0.39       176\n",
      "       AMaj7       0.71      0.59      0.64      1986\n",
      "       AMin7       0.51      0.40      0.45      1234\n",
      "         Ab7       0.28      0.65      0.39       176\n",
      "      AbMaj7       0.72      0.52      0.60      1866\n",
      "      AbMin7       0.57      0.51      0.54      1281\n",
      "          B7       0.23      0.98      0.38       176\n",
      "       BMaj7       0.69      0.57      0.62      1866\n",
      "       BMin7       0.61      0.47      0.53      1512\n",
      "         Bb7       0.32      0.94      0.48       211\n",
      "      BbMaj7       0.74      0.53      0.62      1938\n",
      "      BbMin7       0.56      0.44      0.49      1274\n",
      "          C7       0.32      0.65      0.43       176\n",
      "       CMaj7       0.65      0.57      0.61      2034\n",
      "       CMin7       0.56      0.61      0.58      1234\n",
      "          D7       0.24      0.95      0.38       176\n",
      "       DMaj7       0.71      0.50      0.59      1969\n",
      "       DMin7       0.54      0.50      0.52      1278\n",
      "         Db7       0.19      0.90      0.31       176\n",
      "      DbMaj7       0.59      0.55      0.57      1866\n",
      "      DbMin7       0.45      0.38      0.41      1273\n",
      "          E7       0.23      0.97      0.37       176\n",
      "       EMaj7       0.70      0.57      0.63      1884\n",
      "       EMin7       0.54      0.54      0.54      1375\n",
      "         Eb7       0.19      0.96      0.31       176\n",
      "      EbMaj7       0.66      0.56      0.61      1866\n",
      "      EbMin7       0.57      0.42      0.48      1253\n",
      "          F7       0.23      0.97      0.37       176\n",
      "       FMaj7       0.63      0.58      0.61      1886\n",
      "       FMin7       0.62      0.41      0.49      1234\n",
      "          G7       0.28      0.92      0.42       229\n",
      "       GMaj7       0.72      0.60      0.65      2189\n",
      "       GMin7       0.49      0.52      0.50      1234\n",
      "         Gb7       0.27      0.90      0.41       176\n",
      "      GbMaj7       0.69      0.60      0.65      1975\n",
      "      GbMin7       0.55      0.57      0.56      1234\n",
      "\n",
      "    accuracy                           0.55     40941\n",
      "   macro avg       0.49      0.65      0.50     40941\n",
      "weighted avg       0.61      0.55      0.56     40941\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 15ms/step - accuracy: 0.3703 - loss: 2.4017 - val_accuracy: 0.6049 - val_loss: 1.3760\n",
      "Epoch 2/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 15ms/step - accuracy: 0.4939 - loss: 1.8172 - val_accuracy: 0.6334 - val_loss: 1.2850\n",
      "Epoch 3/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 15ms/step - accuracy: 0.5088 - loss: 1.7435 - val_accuracy: 0.6424 - val_loss: 1.2586\n",
      "Epoch 4/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 15ms/step - accuracy: 0.5190 - loss: 1.7033 - val_accuracy: 0.6340 - val_loss: 1.2699\n",
      "Epoch 5/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 16ms/step - accuracy: 0.5230 - loss: 1.6858 - val_accuracy: 0.6613 - val_loss: 1.2283\n",
      "Epoch 6/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 16ms/step - accuracy: 0.5267 - loss: 1.6658 - val_accuracy: 0.6397 - val_loss: 1.2622\n",
      "Epoch 7/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 16ms/step - accuracy: 0.5299 - loss: 1.6603 - val_accuracy: 0.6603 - val_loss: 1.2320\n",
      "Epoch 8/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 16ms/step - accuracy: 0.5299 - loss: 1.6543 - val_accuracy: 0.6645 - val_loss: 1.2202\n",
      "Epoch 9/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 16ms/step - accuracy: 0.5308 - loss: 1.6496 - val_accuracy: 0.6380 - val_loss: 1.2786\n",
      "Epoch 10/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 16ms/step - accuracy: 0.5333 - loss: 1.6447 - val_accuracy: 0.6535 - val_loss: 1.2207\n",
      "Epoch 11/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 16ms/step - accuracy: 0.5342 - loss: 1.6357 - val_accuracy: 0.6288 - val_loss: 1.2798\n",
      "Epoch 12/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 16ms/step - accuracy: 0.5333 - loss: 1.6382 - val_accuracy: 0.6574 - val_loss: 1.2326\n",
      "Epoch 13/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 16ms/step - accuracy: 0.5348 - loss: 1.6274 - val_accuracy: 0.6463 - val_loss: 1.2593\n",
      "Epoch 14/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 16ms/step - accuracy: 0.5350 - loss: 1.6312 - val_accuracy: 0.6394 - val_loss: 1.2621\n",
      "Epoch 15/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 16ms/step - accuracy: 0.5368 - loss: 1.6323 - val_accuracy: 0.6586 - val_loss: 1.2311\n",
      "Epoch 16/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 16ms/step - accuracy: 0.5383 - loss: 1.6256 - val_accuracy: 0.6603 - val_loss: 1.2414\n",
      "Epoch 17/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 16ms/step - accuracy: 0.5367 - loss: 1.6282 - val_accuracy: 0.6520 - val_loss: 1.2325\n",
      "Epoch 18/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 16ms/step - accuracy: 0.5367 - loss: 1.6231 - val_accuracy: 0.6445 - val_loss: 1.2598\n",
      "::::: model_lstm_16_16_seq_20 :::::\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.5808 - loss: 1.4686\n",
      "[1.706371545791626, 0.5354865789413452]\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A7       0.19      0.94      0.31       176\n",
      "       AMaj7       0.62      0.56      0.59      1986\n",
      "       AMin7       0.50      0.33      0.39      1234\n",
      "         Ab7       0.30      0.90      0.45       176\n",
      "      AbMaj7       0.68      0.58      0.62      1866\n",
      "      AbMin7       0.51      0.53      0.52      1281\n",
      "          B7       0.36      0.83      0.50       176\n",
      "       BMaj7       0.66      0.56      0.61      1866\n",
      "       BMin7       0.63      0.47      0.54      1512\n",
      "         Bb7       0.33      0.92      0.48       211\n",
      "      BbMaj7       0.65      0.50      0.56      1938\n",
      "      BbMin7       0.55      0.46      0.50      1274\n",
      "          C7       0.19      0.89      0.31       176\n",
      "       CMaj7       0.68      0.58      0.63      2034\n",
      "       CMin7       0.61      0.47      0.53      1234\n",
      "          D7       0.20      0.93      0.34       176\n",
      "       DMaj7       0.66      0.52      0.58      1969\n",
      "       DMin7       0.59      0.40      0.48      1268\n",
      "         Db7       0.20      0.91      0.33       176\n",
      "      DbMaj7       0.72      0.51      0.59      1866\n",
      "      DbMin7       0.49      0.47      0.48      1273\n",
      "          E7       0.18      0.97      0.31       176\n",
      "       EMaj7       0.63      0.54      0.58      1884\n",
      "       EMin7       0.62      0.47      0.53      1375\n",
      "         Eb7       0.20      0.95      0.33       176\n",
      "      EbMaj7       0.75      0.51      0.61      1866\n",
      "      EbMin7       0.56      0.56      0.56      1253\n",
      "          F7       0.27      0.97      0.43       176\n",
      "       FMaj7       0.68      0.54      0.60      1886\n",
      "       FMin7       0.62      0.44      0.51      1234\n",
      "          G7       0.20      0.96      0.34       229\n",
      "       GMaj7       0.67      0.48      0.56      2189\n",
      "       GMin7       0.58      0.56      0.57      1234\n",
      "         Gb7       0.21      0.90      0.34       176\n",
      "      GbMaj7       0.66      0.60      0.63      1975\n",
      "      GbMin7       0.53      0.53      0.53      1234\n",
      "\n",
      "    accuracy                           0.54     40931\n",
      "   macro avg       0.49      0.65      0.49     40931\n",
      "weighted avg       0.61      0.54      0.55     40931\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 17ms/step - accuracy: 0.4175 - loss: 2.2461 - val_accuracy: 0.6425 - val_loss: 1.2881\n",
      "Epoch 2/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 17ms/step - accuracy: 0.5346 - loss: 1.6837 - val_accuracy: 0.6564 - val_loss: 1.2635\n",
      "Epoch 3/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 17ms/step - accuracy: 0.5504 - loss: 1.6156 - val_accuracy: 0.6592 - val_loss: 1.2617\n",
      "Epoch 4/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 17ms/step - accuracy: 0.5586 - loss: 1.5738 - val_accuracy: 0.6737 - val_loss: 1.2236\n",
      "Epoch 5/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 17ms/step - accuracy: 0.5625 - loss: 1.5617 - val_accuracy: 0.6753 - val_loss: 1.2199\n",
      "Epoch 6/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 18ms/step - accuracy: 0.5659 - loss: 1.5498 - val_accuracy: 0.6575 - val_loss: 1.2592\n",
      "Epoch 7/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 18ms/step - accuracy: 0.5705 - loss: 1.5256 - val_accuracy: 0.6590 - val_loss: 1.2575\n",
      "Epoch 8/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 18ms/step - accuracy: 0.5732 - loss: 1.5190 - val_accuracy: 0.6674 - val_loss: 1.2444\n",
      "Epoch 9/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 18ms/step - accuracy: 0.5712 - loss: 1.5187 - val_accuracy: 0.6770 - val_loss: 1.2288\n",
      "Epoch 10/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 18ms/step - accuracy: 0.5751 - loss: 1.5103 - val_accuracy: 0.6814 - val_loss: 1.2283\n",
      "Epoch 11/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 18ms/step - accuracy: 0.5764 - loss: 1.5054 - val_accuracy: 0.6790 - val_loss: 1.2208\n",
      "Epoch 12/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 18ms/step - accuracy: 0.5765 - loss: 1.4980 - val_accuracy: 0.6811 - val_loss: 1.2170\n",
      "Epoch 13/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 18ms/step - accuracy: 0.5775 - loss: 1.5010 - val_accuracy: 0.6741 - val_loss: 1.2409\n",
      "Epoch 14/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 18ms/step - accuracy: 0.5783 - loss: 1.4912 - val_accuracy: 0.6881 - val_loss: 1.2093\n",
      "Epoch 15/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 18ms/step - accuracy: 0.5799 - loss: 1.4924 - val_accuracy: 0.6677 - val_loss: 1.2568\n",
      "Epoch 16/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 18ms/step - accuracy: 0.5796 - loss: 1.4892 - val_accuracy: 0.6691 - val_loss: 1.2594\n",
      "Epoch 17/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 18ms/step - accuracy: 0.5807 - loss: 1.4862 - val_accuracy: 0.6403 - val_loss: 1.3121\n",
      "Epoch 18/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 18ms/step - accuracy: 0.5837 - loss: 1.4850 - val_accuracy: 0.6631 - val_loss: 1.2637\n",
      "Epoch 19/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 18ms/step - accuracy: 0.5830 - loss: 1.4784 - val_accuracy: 0.6892 - val_loss: 1.2267\n",
      "Epoch 20/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 18ms/step - accuracy: 0.5803 - loss: 1.4893 - val_accuracy: 0.6626 - val_loss: 1.2519\n",
      "Epoch 21/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 18ms/step - accuracy: 0.5818 - loss: 1.4898 - val_accuracy: 0.6774 - val_loss: 1.2260\n",
      "Epoch 22/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 19ms/step - accuracy: 0.5838 - loss: 1.4787 - val_accuracy: 0.6689 - val_loss: 1.2521\n",
      "Epoch 23/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 24ms/step - accuracy: 0.5831 - loss: 1.4795 - val_accuracy: 0.6698 - val_loss: 1.2483\n",
      "Epoch 24/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 28ms/step - accuracy: 0.5856 - loss: 1.4728 - val_accuracy: 0.6625 - val_loss: 1.2649\n",
      "::::: model_lstm_32_32_seq_20 :::::\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.5808 - loss: 1.4780\n",
      "[1.7101112604141235, 0.5439153909683228]\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A7       0.25      0.94      0.39       176\n",
      "       AMaj7       0.67      0.54      0.60      1986\n",
      "       AMin7       0.48      0.46      0.47      1234\n",
      "         Ab7       0.33      0.89      0.48       176\n",
      "      AbMaj7       0.74      0.50      0.60      1866\n",
      "      AbMin7       0.58      0.44      0.50      1281\n",
      "          B7       0.29      0.85      0.43       176\n",
      "       BMaj7       0.66      0.59      0.62      1866\n",
      "       BMin7       0.56      0.50      0.53      1512\n",
      "         Bb7       0.28      0.98      0.43       211\n",
      "      BbMaj7       0.62      0.50      0.55      1938\n",
      "      BbMin7       0.53      0.51      0.52      1274\n",
      "          C7       0.24      0.86      0.38       176\n",
      "       CMaj7       0.63      0.58      0.61      2034\n",
      "       CMin7       0.61      0.54      0.57      1234\n",
      "          D7       0.23      0.93      0.37       176\n",
      "       DMaj7       0.71      0.52      0.60      1969\n",
      "       DMin7       0.52      0.53      0.53      1268\n",
      "         Db7       0.21      0.85      0.34       176\n",
      "      DbMaj7       0.63      0.48      0.55      1866\n",
      "      DbMin7       0.50      0.44      0.47      1273\n",
      "          E7       0.20      0.94      0.33       176\n",
      "       EMaj7       0.61      0.58      0.60      1884\n",
      "       EMin7       0.53      0.47      0.50      1375\n",
      "         Eb7       0.25      0.96      0.40       176\n",
      "      EbMaj7       0.68      0.59      0.63      1866\n",
      "      EbMin7       0.60      0.49      0.54      1253\n",
      "          F7       0.27      0.97      0.43       176\n",
      "       FMaj7       0.71      0.51      0.59      1886\n",
      "       FMin7       0.55      0.46      0.50      1234\n",
      "          G7       0.23      0.87      0.36       229\n",
      "       GMaj7       0.67      0.62      0.64      2189\n",
      "       GMin7       0.55      0.43      0.48      1234\n",
      "         Gb7       0.22      0.91      0.36       176\n",
      "      GbMaj7       0.72      0.56      0.63      1975\n",
      "      GbMin7       0.56      0.53      0.54      1234\n",
      "\n",
      "    accuracy                           0.54     40931\n",
      "   macro avg       0.49      0.65      0.50     40931\n",
      "weighted avg       0.60      0.54      0.56     40931\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 20ms/step - accuracy: 0.4540 - loss: 2.1363 - val_accuracy: 0.6265 - val_loss: 1.4038\n",
      "Epoch 2/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 21ms/step - accuracy: 0.5551 - loss: 1.6629 - val_accuracy: 0.6533 - val_loss: 1.3311\n",
      "Epoch 3/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m249s\u001b[0m 21ms/step - accuracy: 0.5707 - loss: 1.5946 - val_accuracy: 0.6441 - val_loss: 1.3732\n",
      "Epoch 4/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 21ms/step - accuracy: 0.5774 - loss: 1.5637 - val_accuracy: 0.6646 - val_loss: 1.3265\n",
      "Epoch 5/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 21ms/step - accuracy: 0.5808 - loss: 1.5482 - val_accuracy: 0.6443 - val_loss: 1.3663\n",
      "Epoch 6/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 21ms/step - accuracy: 0.5858 - loss: 1.5286 - val_accuracy: 0.6656 - val_loss: 1.3168\n",
      "Epoch 7/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 21ms/step - accuracy: 0.5889 - loss: 1.5184 - val_accuracy: 0.6476 - val_loss: 1.3573\n",
      "Epoch 8/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 21ms/step - accuracy: 0.5911 - loss: 1.5110 - val_accuracy: 0.6751 - val_loss: 1.3129\n",
      "Epoch 9/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 21ms/step - accuracy: 0.5930 - loss: 1.4957 - val_accuracy: 0.6648 - val_loss: 1.3170\n",
      "Epoch 10/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 21ms/step - accuracy: 0.5967 - loss: 1.4929 - val_accuracy: 0.6897 - val_loss: 1.2592\n",
      "Epoch 11/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 21ms/step - accuracy: 0.5968 - loss: 1.4862 - val_accuracy: 0.6618 - val_loss: 1.3168\n",
      "Epoch 12/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 21ms/step - accuracy: 0.5986 - loss: 1.4716 - val_accuracy: 0.6639 - val_loss: 1.3246\n",
      "Epoch 13/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 21ms/step - accuracy: 0.5998 - loss: 1.4629 - val_accuracy: 0.6702 - val_loss: 1.3063\n",
      "Epoch 14/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 21ms/step - accuracy: 0.6013 - loss: 1.4676 - val_accuracy: 0.6607 - val_loss: 1.3332\n",
      "Epoch 15/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 21ms/step - accuracy: 0.6014 - loss: 1.4602 - val_accuracy: 0.6771 - val_loss: 1.2897\n",
      "Epoch 16/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 21ms/step - accuracy: 0.6040 - loss: 1.4516 - val_accuracy: 0.6553 - val_loss: 1.3434\n",
      "Epoch 17/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 22ms/step - accuracy: 0.6034 - loss: 1.4581 - val_accuracy: 0.6795 - val_loss: 1.2874\n",
      "Epoch 18/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 22ms/step - accuracy: 0.6024 - loss: 1.4530 - val_accuracy: 0.6350 - val_loss: 1.3860\n",
      "Epoch 19/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 22ms/step - accuracy: 0.6064 - loss: 1.4415 - val_accuracy: 0.6556 - val_loss: 1.3346\n",
      "Epoch 20/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 22ms/step - accuracy: 0.6063 - loss: 1.4404 - val_accuracy: 0.6799 - val_loss: 1.2853\n",
      "::::: model_lstm_64_64_seq_20 :::::\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.5969 - loss: 1.5507\n",
      "[1.8027032613754272, 0.5508538484573364]\n",
      "\u001b[1m1280/1280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A7       0.22      0.92      0.35       176\n",
      "       AMaj7       0.69      0.55      0.62      1986\n",
      "       AMin7       0.51      0.47      0.49      1234\n",
      "         Ab7       0.22      0.93      0.35       176\n",
      "      AbMaj7       0.71      0.56      0.63      1866\n",
      "      AbMin7       0.53      0.53      0.53      1281\n",
      "          B7       0.24      0.93      0.38       176\n",
      "       BMaj7       0.72      0.50      0.59      1866\n",
      "       BMin7       0.54      0.59      0.56      1512\n",
      "         Bb7       0.53      0.73      0.62       211\n",
      "      BbMaj7       0.66      0.56      0.61      1938\n",
      "      BbMin7       0.53      0.44      0.48      1274\n",
      "          C7       0.29      0.86      0.44       176\n",
      "       CMaj7       0.62      0.59      0.60      2034\n",
      "       CMin7       0.54      0.58      0.56      1234\n",
      "          D7       0.22      0.92      0.36       176\n",
      "       DMaj7       0.68      0.55      0.61      1969\n",
      "       DMin7       0.54      0.54      0.54      1268\n",
      "         Db7       0.22      0.92      0.35       176\n",
      "      DbMaj7       0.65      0.48      0.55      1866\n",
      "      DbMin7       0.50      0.49      0.49      1273\n",
      "          E7       0.29      0.94      0.44       176\n",
      "       EMaj7       0.66      0.54      0.59      1884\n",
      "       EMin7       0.56      0.54      0.55      1375\n",
      "         Eb7       0.25      0.95      0.39       176\n",
      "      EbMaj7       0.68      0.56      0.62      1866\n",
      "      EbMin7       0.56      0.48      0.52      1253\n",
      "          F7       0.20      0.94      0.33       176\n",
      "       FMaj7       0.68      0.50      0.58      1886\n",
      "       FMin7       0.56      0.35      0.43      1234\n",
      "          G7       0.33      0.90      0.48       229\n",
      "       GMaj7       0.72      0.61      0.66      2189\n",
      "       GMin7       0.56      0.49      0.52      1234\n",
      "         Gb7       0.26      0.92      0.41       176\n",
      "      GbMaj7       0.70      0.60      0.64      1975\n",
      "      GbMin7       0.58      0.48      0.53      1234\n",
      "\n",
      "    accuracy                           0.55     40931\n",
      "   macro avg       0.50      0.65      0.51     40931\n",
      "weighted avg       0.61      0.55      0.56     40931\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    (16, 16),\n",
    "    (32, 32),\n",
    "    (64, 64)\n",
    "]\n",
    "\n",
    "seq_lens = [5, 10, 20]\n",
    "    \n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    # monitor=\"val_accuracy\"\n",
    ")\n",
    "\n",
    "def make_seq(X, y_encoded, seq_len):\n",
    "    X_seq, y_encoded_seq = None, None\n",
    "    X_seq_list = []\n",
    "    y_encoded_seq_list = []\n",
    "    for i in range(len(X) - seq_len + 1):\n",
    "        X_seq_list.append(X.values[i : i + seq_len, :])\n",
    "        y_encoded_seq_list.append(y_encoded[i + seq_len - 1])\n",
    "    \n",
    "    return np.array(X_seq_list), np.array(y_encoded_seq_list)\n",
    "\n",
    "# models = []\n",
    "for seq_len in seq_lens:\n",
    "    X_train_seq, y_train_encoded_seq = make_seq(X_train, y_train_encoded, seq_len)\n",
    "    X_val_seq, y_val_encoded_seq = make_seq(X_val, y_val_encoded, seq_len)\n",
    "    X_test_seq, y_test_encoded_seq = make_seq(X_test, y_test_encoded, seq_len)\n",
    "    \n",
    "    for layer_1, layer_2 in layers:\n",
    "        model_lstm = tf.keras.Sequential([\n",
    "            # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True, input_shape=(SEQUENCE_LEN, X_seq_train.shape[2]))),\n",
    "            tf.keras.layers.Bidirectional(\n",
    "                tf.keras.layers.LSTM(\n",
    "                    layer_1,\n",
    "                    return_sequences=True,\n",
    "                    input_shape=(seq_len, X_train_seq.shape[2]),\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(1e-3),\n",
    "                    recurrent_regularizer=tf.keras.regularizers.l2(1e-3),\n",
    "                    recurrent_dropout=0.25,\n",
    "                )\n",
    "            ),\n",
    "            tf.keras.layers.Dropout(0.25),\n",
    "                \n",
    "            tf.keras.layers.Bidirectional(\n",
    "                tf.keras.layers.LSTM(\n",
    "                    layer_2,\n",
    "                    return_sequences=False,\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(1e-3),\n",
    "                    recurrent_regularizer=tf.keras.regularizers.l2(1e-3),\n",
    "                    recurrent_dropout=0.25,\n",
    "                )\n",
    "            ),\n",
    "            tf.keras.layers.Dropout(0.25),\n",
    "        \n",
    "            tf.keras.layers.Dense(\n",
    "                len(encoder.classes_),\n",
    "                activation='softmax',\n",
    "                # kernel_regularizer=tf.keras.regularizers.l2(1e-2)\n",
    "            ),\n",
    "        ])\n",
    "        \n",
    "        model_lstm.compile(\n",
    "            optimizer=\"adam\",\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        \n",
    "        # model_lstm.summary()\n",
    "        \n",
    "        history = model_lstm.fit(\n",
    "            # X_seq_train,\n",
    "            # y_seq_train,\n",
    "            X_train_seq,\n",
    "            y_train_encoded_seq,\n",
    "            epochs=500,\n",
    "            batch_size=32,\n",
    "            # validation_split=0.1,\n",
    "            validation_data=(X_val_seq, y_val_encoded_seq),\n",
    "            verbose=1,\n",
    "            callbacks=[early_stopping],\n",
    "            class_weight=class_weight_dict,\n",
    "        )\n",
    "    \n",
    "        # models.append(model_lstm)\n",
    "        \n",
    "        model_lstm.save(f\"./Models/model_lstm_{layer_1}_{layer_2}_seq_{seq_len}.keras\")\n",
    "        pd.DataFrame(history.history).to_csv(f\"./History/model_lstm_{layer_1}_{layer_2}_seq_{seq_len}_history.csv\", index=False)\n",
    "\n",
    "        print(f\"::::: model_lstm_{layer_1}_{layer_2}_seq_{seq_len} :::::\")\n",
    "        print(model_lstm.evaluate(X_test_seq, y_test_encoded_seq))\n",
    "        y_pred = np.argmax(model_lstm.predict(X_test_seq), axis=1)\n",
    "        print(\n",
    "            sk.metrics.classification_report(y_test_encoded_seq, y_pred, target_names=encoder.classes_)\n",
    "        )\n",
    "        print(\"\")\n",
    "    \n",
    "        gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_venv",
   "language": "python",
   "name": "thesis_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
