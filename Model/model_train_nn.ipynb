{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a087c41a-4373-4ec1-972b-e8e8899669ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":::::::::::::::::::: PREPARING ::::::::::::::::::::\n",
      "[INFO]: Featurizing 45 files: \n",
      "  Featurizing 1.mp3 -> .........................DONE\n",
      "  Featurizing 10.mp3 -> .........................DONE\n",
      "  Featurizing 11.mp3 -> .........................DONE\n",
      "  Featurizing 12.mp3 -> .........................DONE\n",
      "  Featurizing 13.mp3 -> .........................DONE\n",
      "  Featurizing 14.mp3 -> .........................DONE\n",
      "  Featurizing 15.mp3 -> .........................DONE\n",
      "  Featurizing 16.mp3 -> .........................DONE\n",
      "  Featurizing 17.mp3 -> .........................DONE\n",
      "  Featurizing 18.mp3 -> .........................DONE\n",
      "  Featurizing 19.mp3 -> .........................DONE\n",
      "  Featurizing 2.mp3 -> .........................DONE\n",
      "  Featurizing 20.mp3 -> .........................DONE\n",
      "  Featurizing 21.mp3 -> .........................DONE\n",
      "  Featurizing 22.mp3 -> .........................DONE\n",
      "  Featurizing 23.mp3 -> .........................DONE\n",
      "  Featurizing 24.mp3 -> .........................DONE\n",
      "  Featurizing 25.mp3 -> .........................DONE\n",
      "  Featurizing 26.mp3 -> .........................DONE\n",
      "  Featurizing 27.mp3 -> .........................DONE\n",
      "  Featurizing 28.mp3 -> .........................DONE\n",
      "  Featurizing 29.mp3 -> .........................DONE\n",
      "  Featurizing 3.mp3 -> .........................DONE\n",
      "  Featurizing 30.mp3 -> .........................DONE\n",
      "  Featurizing 31.mp3 -> .........................DONE\n",
      "  Featurizing 32.mp3 -> .........................DONE\n",
      "  Featurizing 33.mp3 -> .........................DONE\n",
      "  Featurizing 34.mp3 -> .........................DONE\n",
      "  Featurizing 35.mp3 -> .........................DONE\n",
      "  Featurizing 36.mp3 -> .........................DONE\n",
      "  Featurizing 37.mp3 -> .........................DONE\n",
      "  Featurizing 38.mp3 -> .........................DONE\n",
      "  Featurizing 39.mp3 -> .........................DONE\n",
      "  Featurizing 4.mp3 -> .........................DONE\n",
      "  Featurizing 40.mp3 -> .........................DONE\n",
      "  Featurizing 41.mp3 -> .........................DONE\n",
      "  Featurizing 42.mp3 -> .........................DONE\n",
      "  Featurizing 43.mp3 -> .........................DONE\n",
      "  Featurizing 44.mp3 -> .........................DONE\n",
      "  Featurizing 45.mp3 -> .........................DONE\n",
      "  Featurizing 5.mp3 -> .........................DONE\n",
      "  Featurizing 6.mp3 -> .........................DONE\n",
      "  Featurizing 7.mp3 -> .........................DONE\n",
      "  Featurizing 8.mp3 -> .........................DONE\n",
      "  Featurizing 9.mp3 -> .........................DONE\n",
      "[INFO]: Total segments -> 491975\n",
      "[INFO]: Total chords   -> 36\n",
      "[INFO]: Chords labels ['FMin7' 'G7' 'CMin7' 'AbMaj7' 'GbMin7' 'Ab7' 'DbMin7' 'AMaj7' 'GMin7'\n",
      " 'A7' 'DMin7' 'BbMaj7' 'AbMin7' 'Bb7' 'EbMin7' 'BMaj7' 'AMin7' 'B7'\n",
      " 'EMin7' 'CMaj7' 'BbMin7' 'C7' 'DbMaj7' 'BMin7' 'Db7' 'DMaj7' 'D7'\n",
      " 'EbMaj7' 'Eb7' 'EMaj7' 'E7' 'FMaj7' 'F7' 'GbMaj7' 'Gb7' 'GMaj7']\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ./process_data_new.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a86b41fd-04c1-483c-b2fc-56926d6c3707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 21:15:08.637696: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import joblib\n",
    "import random\n",
    "import gc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd50646b-5ad4-4ade-93b2-0b42201c6398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9bdfa92-78f8-4b7c-b19e-8d2b81782799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cens_C</th>\n",
       "      <th>Cens_Db</th>\n",
       "      <th>Cens_D</th>\n",
       "      <th>Cens_Eb</th>\n",
       "      <th>Cens_E</th>\n",
       "      <th>Cens_F</th>\n",
       "      <th>Cens_Gb</th>\n",
       "      <th>Cens_G</th>\n",
       "      <th>Cens_Ab</th>\n",
       "      <th>Cens_A</th>\n",
       "      <th>Cens_Bb</th>\n",
       "      <th>Cens_B</th>\n",
       "      <th>chord</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.458163</td>\n",
       "      <td>0.228498</td>\n",
       "      <td>0.335618</td>\n",
       "      <td>0.385245</td>\n",
       "      <td>0.280831</td>\n",
       "      <td>0.562060</td>\n",
       "      <td>0.101404</td>\n",
       "      <td>0.142517</td>\n",
       "      <td>0.211683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081103</td>\n",
       "      <td>0.007991</td>\n",
       "      <td>FMin7</td>\n",
       "      <td>../Datasets/1.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.456882</td>\n",
       "      <td>0.228481</td>\n",
       "      <td>0.329903</td>\n",
       "      <td>0.386718</td>\n",
       "      <td>0.270461</td>\n",
       "      <td>0.563033</td>\n",
       "      <td>0.104248</td>\n",
       "      <td>0.142325</td>\n",
       "      <td>0.223759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096144</td>\n",
       "      <td>0.008832</td>\n",
       "      <td>FMin7</td>\n",
       "      <td>../Datasets/1.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.454308</td>\n",
       "      <td>0.229081</td>\n",
       "      <td>0.324611</td>\n",
       "      <td>0.388377</td>\n",
       "      <td>0.260669</td>\n",
       "      <td>0.563478</td>\n",
       "      <td>0.107217</td>\n",
       "      <td>0.142526</td>\n",
       "      <td>0.235052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110957</td>\n",
       "      <td>0.009594</td>\n",
       "      <td>FMin7</td>\n",
       "      <td>../Datasets/1.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.450574</td>\n",
       "      <td>0.230340</td>\n",
       "      <td>0.319815</td>\n",
       "      <td>0.390179</td>\n",
       "      <td>0.251621</td>\n",
       "      <td>0.563336</td>\n",
       "      <td>0.110333</td>\n",
       "      <td>0.143008</td>\n",
       "      <td>0.245606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125269</td>\n",
       "      <td>0.010269</td>\n",
       "      <td>FMin7</td>\n",
       "      <td>../Datasets/1.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.445894</td>\n",
       "      <td>0.232287</td>\n",
       "      <td>0.315578</td>\n",
       "      <td>0.392097</td>\n",
       "      <td>0.243367</td>\n",
       "      <td>0.562581</td>\n",
       "      <td>0.113524</td>\n",
       "      <td>0.143594</td>\n",
       "      <td>0.255459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138864</td>\n",
       "      <td>0.010856</td>\n",
       "      <td>FMin7</td>\n",
       "      <td>../Datasets/1.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.207725</td>\n",
       "      <td>0.076254</td>\n",
       "      <td>0.397630</td>\n",
       "      <td>0.012440</td>\n",
       "      <td>0.081124</td>\n",
       "      <td>0.025550</td>\n",
       "      <td>0.113143</td>\n",
       "      <td>0.602074</td>\n",
       "      <td>0.253705</td>\n",
       "      <td>0.415162</td>\n",
       "      <td>0.012520</td>\n",
       "      <td>0.416366</td>\n",
       "      <td>AMin7</td>\n",
       "      <td>../Datasets/9.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.200846</td>\n",
       "      <td>0.072857</td>\n",
       "      <td>0.392787</td>\n",
       "      <td>0.013882</td>\n",
       "      <td>0.080756</td>\n",
       "      <td>0.028333</td>\n",
       "      <td>0.115661</td>\n",
       "      <td>0.609691</td>\n",
       "      <td>0.257939</td>\n",
       "      <td>0.416193</td>\n",
       "      <td>0.013882</td>\n",
       "      <td>0.409283</td>\n",
       "      <td>AMin7</td>\n",
       "      <td>../Datasets/9.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.194574</td>\n",
       "      <td>0.068975</td>\n",
       "      <td>0.387773</td>\n",
       "      <td>0.015407</td>\n",
       "      <td>0.080133</td>\n",
       "      <td>0.031256</td>\n",
       "      <td>0.117944</td>\n",
       "      <td>0.616832</td>\n",
       "      <td>0.261322</td>\n",
       "      <td>0.417602</td>\n",
       "      <td>0.015407</td>\n",
       "      <td>0.402578</td>\n",
       "      <td>AMin7</td>\n",
       "      <td>../Datasets/9.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.188929</td>\n",
       "      <td>0.064633</td>\n",
       "      <td>0.382609</td>\n",
       "      <td>0.017022</td>\n",
       "      <td>0.079286</td>\n",
       "      <td>0.034334</td>\n",
       "      <td>0.119918</td>\n",
       "      <td>0.623479</td>\n",
       "      <td>0.263771</td>\n",
       "      <td>0.419261</td>\n",
       "      <td>0.017022</td>\n",
       "      <td>0.396526</td>\n",
       "      <td>AMin7</td>\n",
       "      <td>../Datasets/9.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.183925</td>\n",
       "      <td>0.059857</td>\n",
       "      <td>0.377302</td>\n",
       "      <td>0.018739</td>\n",
       "      <td>0.078254</td>\n",
       "      <td>0.037583</td>\n",
       "      <td>0.121490</td>\n",
       "      <td>0.629592</td>\n",
       "      <td>0.265280</td>\n",
       "      <td>0.421086</td>\n",
       "      <td>0.018739</td>\n",
       "      <td>0.391356</td>\n",
       "      <td>AMin7</td>\n",
       "      <td>../Datasets/9.mp3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>491975 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Cens_C   Cens_Db    Cens_D   Cens_Eb    Cens_E    Cens_F   Cens_Gb  \\\n",
       "0    0.458163  0.228498  0.335618  0.385245  0.280831  0.562060  0.101404   \n",
       "1    0.456882  0.228481  0.329903  0.386718  0.270461  0.563033  0.104248   \n",
       "2    0.454308  0.229081  0.324611  0.388377  0.260669  0.563478  0.107217   \n",
       "3    0.450574  0.230340  0.319815  0.390179  0.251621  0.563336  0.110333   \n",
       "4    0.445894  0.232287  0.315578  0.392097  0.243367  0.562581  0.113524   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "108  0.207725  0.076254  0.397630  0.012440  0.081124  0.025550  0.113143   \n",
       "109  0.200846  0.072857  0.392787  0.013882  0.080756  0.028333  0.115661   \n",
       "110  0.194574  0.068975  0.387773  0.015407  0.080133  0.031256  0.117944   \n",
       "111  0.188929  0.064633  0.382609  0.017022  0.079286  0.034334  0.119918   \n",
       "112  0.183925  0.059857  0.377302  0.018739  0.078254  0.037583  0.121490   \n",
       "\n",
       "       Cens_G   Cens_Ab    Cens_A   Cens_Bb    Cens_B  chord  \\\n",
       "0    0.142517  0.211683  0.000000  0.081103  0.007991  FMin7   \n",
       "1    0.142325  0.223759  0.000000  0.096144  0.008832  FMin7   \n",
       "2    0.142526  0.235052  0.000000  0.110957  0.009594  FMin7   \n",
       "3    0.143008  0.245606  0.000000  0.125269  0.010269  FMin7   \n",
       "4    0.143594  0.255459  0.000000  0.138864  0.010856  FMin7   \n",
       "..        ...       ...       ...       ...       ...    ...   \n",
       "108  0.602074  0.253705  0.415162  0.012520  0.416366  AMin7   \n",
       "109  0.609691  0.257939  0.416193  0.013882  0.409283  AMin7   \n",
       "110  0.616832  0.261322  0.417602  0.015407  0.402578  AMin7   \n",
       "111  0.623479  0.263771  0.419261  0.017022  0.396526  AMin7   \n",
       "112  0.629592  0.265280  0.421086  0.018739  0.391356  AMin7   \n",
       "\n",
       "              filename  \n",
       "0    ../Datasets/1.mp3  \n",
       "1    ../Datasets/1.mp3  \n",
       "2    ../Datasets/1.mp3  \n",
       "3    ../Datasets/1.mp3  \n",
       "4    ../Datasets/1.mp3  \n",
       "..                 ...  \n",
       "108  ../Datasets/9.mp3  \n",
       "109  ../Datasets/9.mp3  \n",
       "110  ../Datasets/9.mp3  \n",
       "111  ../Datasets/9.mp3  \n",
       "112  ../Datasets/9.mp3  \n",
       "\n",
       "[491975 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_hdf(\"./dataset.h5\", key=\"df\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db232803-2ce8-4648-ad85-b5a16bb05e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_file = df[\"filename\"].unique()\n",
    "train_file, test_file = sk.model_selection.train_test_split(\n",
    "    unique_file,\n",
    "    random_state=42,\n",
    "    test_size=0.2,\n",
    ")\n",
    "test_file, val_file = sk.model_selection.train_test_split(\n",
    "    train_file,\n",
    "    random_state=42,\n",
    "    test_size=0.5,\n",
    ")\n",
    "\n",
    "df_train = df[df[\"filename\"].isin(train_file)]\n",
    "df_val = df[df[\"filename\"].isin(val_file)]\n",
    "df_test = df[df[\"filename\"].isin(test_file)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "155fb292-42c6-4743-bf2d-4325516940ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chords\n",
      "\n",
      "## TRAIN ## chord\n",
      "AbMin7    15671\n",
      "CMin7     15642\n",
      "FMin7     15555\n",
      "GbMin7    15474\n",
      "DbMin7    15298\n",
      "EMin7     14917\n",
      "AMin7     14859\n",
      "BbMin7    14805\n",
      "GMin7     14679\n",
      "EbMin7    14666\n",
      "DMin7     14658\n",
      "BMin7     14601\n",
      "AMaj7     14174\n",
      "AbMaj7    13936\n",
      "EbMaj7    13895\n",
      "EMaj7     13889\n",
      "BbMaj7    13815\n",
      "GbMaj7    13661\n",
      "BMaj7     13602\n",
      "GMaj7     13564\n",
      "DbMaj7    13555\n",
      "DMaj7     13547\n",
      "CMaj7     13523\n",
      "FMaj7     13339\n",
      "G7         3454\n",
      "C7         3395\n",
      "Bb7        3386\n",
      "B7         3360\n",
      "Db7        3232\n",
      "Ab7        3230\n",
      "D7         3200\n",
      "Eb7        3185\n",
      "E7         3175\n",
      "A7         3162\n",
      "Gb7        3155\n",
      "F7         3141\n",
      "Name: count, dtype: int64\n",
      "\n",
      "## VAL ## chord\n",
      "AMaj7     9935\n",
      "EbMaj7    9907\n",
      "EMaj7     9867\n",
      "BbMaj7    9819\n",
      "AbMaj7    9812\n",
      "BMaj7     9731\n",
      "GMaj7     9713\n",
      "GbMaj7    9658\n",
      "DMaj7     9651\n",
      "CMaj7     9531\n",
      "DbMaj7    9529\n",
      "FMaj7     9472\n",
      "AbMin7    9040\n",
      "CMin7     9006\n",
      "FMin7     8968\n",
      "GbMin7    8910\n",
      "DbMin7    8722\n",
      "AMin7     8683\n",
      "EMin7     8678\n",
      "BbMin7    8492\n",
      "GMin7     8455\n",
      "EbMin7    8426\n",
      "BMin7     8409\n",
      "DMin7     8386\n",
      "C7        2202\n",
      "B7        2197\n",
      "Bb7       2164\n",
      "G7        2135\n",
      "Db7       2070\n",
      "Gb7       2005\n",
      "E7        1999\n",
      "Ab7       1998\n",
      "D7        1986\n",
      "A7        1985\n",
      "Eb7       1983\n",
      "F7        1976\n",
      "Name: count, dtype: int64\n",
      "\n",
      "## TEST ## chord\n",
      "CMin7     6636\n",
      "AbMin7    6631\n",
      "FMin7     6587\n",
      "DbMin7    6576\n",
      "GbMin7    6564\n",
      "BbMin7    6313\n",
      "DMin7     6272\n",
      "EbMin7    6240\n",
      "EMin7     6239\n",
      "GMin7     6224\n",
      "BMin7     6192\n",
      "AMin7     6176\n",
      "AMaj7     4239\n",
      "AbMaj7    4124\n",
      "DbMaj7    4026\n",
      "EMaj7     4022\n",
      "GbMaj7    4003\n",
      "BbMaj7    3996\n",
      "CMaj7     3992\n",
      "EbMaj7    3988\n",
      "DMaj7     3896\n",
      "BMaj7     3871\n",
      "FMaj7     3867\n",
      "GMaj7     3851\n",
      "G7        1319\n",
      "Ab7       1232\n",
      "Bb7       1222\n",
      "D7        1214\n",
      "Eb7       1202\n",
      "C7        1193\n",
      "A7        1177\n",
      "E7        1176\n",
      "F7        1165\n",
      "B7        1163\n",
      "Db7       1162\n",
      "Gb7       1150\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Chords\")\n",
    "print(\"\\n## TRAIN ##\", df_train[\"chord\"].value_counts())\n",
    "print(\"\\n## VAL ##\", df_val[\"chord\"].value_counts())\n",
    "print(\"\\n## TEST ##\", df_test[\"chord\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f22910a2-a2b3-4672-8409-0d9335166d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./encoder.xz']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y = df[\"chord\"]\n",
    "# X = df.drop(columns=\"chord\")\n",
    "# encoder = sk.preprocessing.LabelEncoder()\n",
    "# y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "# joblib.dump(encoder, \"./encoder.xz\")\n",
    "\n",
    "def get_Xy(df: pd.DataFrame):\n",
    "    return df.drop([\"chord\", \"filename\"], axis=1), df[\"chord\"]\n",
    "\n",
    "X_train, y_train = get_Xy(df_train)\n",
    "X_val, y_val = get_Xy(df_val)\n",
    "X_test, y_test = get_Xy(df_test)\n",
    "\n",
    "encoder = sk.preprocessing.LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "y_val_encoded = encoder.transform(y_val)\n",
    "y_test_encoded = encoder.transform(y_test)\n",
    "\n",
    "joblib.dump(encoder, \"./encoder.xz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01f9d408-e165-4e68-941f-ccdcd4c36b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del df\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb64354-b100-49f1-bce9-dc17f43e08b2",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a22ecbda-f306-4fea-9746-7d4ca17726fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEQUENCE_LEN = 20 # 0.1 * 20.0 = 2 sec of sequence data\n",
    "\n",
    "# def make_seq(X, y_encoded):\n",
    "#     X_seq, y_encoded_seq = None, None\n",
    "#     X_seq_list = []\n",
    "#     y_encoded_seq_list = []\n",
    "#     for i in range(len(X) - SEQUENCE_LEN + 1):\n",
    "#         X_seq_list.append(X.values[i : i + SEQUENCE_LEN, :])\n",
    "#         y_encoded_seq_list.append(y_encoded[i + SEQUENCE_LEN - 1])\n",
    "    \n",
    "#     return np.array(X_seq_list), np.array(y_encoded_seq_list)\n",
    "\n",
    "# X_train, y_train_encoded = make_seq(X_train, y_train_encoded)\n",
    "# X_val, y_val_encoded = make_seq(X_val, y_val_encoded)\n",
    "# X_test, y_test_encoded = make_seq(X_test, y_test_encoded)\n",
    "\n",
    "# print(\"TRAIN\")\n",
    "# print(\"X sequence shape: \", X_train.shape)\n",
    "# print(\"y sequence shape: \", y_train_encoded.shape)\n",
    "\n",
    "# print(\"\\nVAL\")\n",
    "# print(\"X sequence shape: \", X_val.shape)\n",
    "# print(\"y sequence shape: \", y_val_encoded.shape)\n",
    "\n",
    "# print(\"\\nTEST\")\n",
    "# print(\"X sequence shape: \", X_test.shape)\n",
    "# print(\"y sequence shape: \", y_test_encoded.shape)\n",
    "\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b2ec4ba-ec19-4210-b01d-bbc116eac1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_seq_train, X_seq_test, y_seq_train, y_seq_test = sk.model_selection.train_test_split(\n",
    "#     X_seq,\n",
    "#     y_encoded_seq,\n",
    "#     test_size=0.2,\n",
    "#     random_state=42,\n",
    "#     stratify=y_encoded_seq,\n",
    "# )\n",
    "\n",
    "# print(f\"X_train: {len(X_seq_train)}\")\n",
    "# print(f\"y_train: {len(y_seq_train)}\")\n",
    "# print(f\"X_test:  {len(X_seq_test)}\")\n",
    "# print(f\"y_test:  {len(y_seq_test)}\")\n",
    "\n",
    "# del X_seq\n",
    "# del y_encoded_seq\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57f6e917-5e8b-49c4-bb6f-97258dfa85e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Total feature: \", X_train.shape[2])\n",
    "# print(\"Total class:   \", len(encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bce6e562-b4ae-4bde-bc29-4b1f70ce3550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = sk.utils.class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train,\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adb2f230-0a9d-455c-86f4-4fb5ac20590a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 8ms/step - accuracy: 0.4063 - loss: 2.2756 - val_accuracy: 0.5487 - val_loss: 1.6238\n",
      "Epoch 2/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 8ms/step - accuracy: 0.5085 - loss: 1.7623 - val_accuracy: 0.5500 - val_loss: 1.5868\n",
      "Epoch 3/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 8ms/step - accuracy: 0.5161 - loss: 1.7189 - val_accuracy: 0.5611 - val_loss: 1.5509\n",
      "Epoch 4/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 8ms/step - accuracy: 0.5240 - loss: 1.6945 - val_accuracy: 0.5648 - val_loss: 1.5316\n",
      "Epoch 5/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 8ms/step - accuracy: 0.5267 - loss: 1.6843 - val_accuracy: 0.5605 - val_loss: 1.5333\n",
      "Epoch 6/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 7ms/step - accuracy: 0.5280 - loss: 1.6794 - val_accuracy: 0.5672 - val_loss: 1.5186\n",
      "Epoch 7/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 8ms/step - accuracy: 0.5316 - loss: 1.6682 - val_accuracy: 0.5587 - val_loss: 1.5372\n",
      "Epoch 8/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 8ms/step - accuracy: 0.5302 - loss: 1.6584 - val_accuracy: 0.5638 - val_loss: 1.5214\n",
      "Epoch 9/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 8ms/step - accuracy: 0.5321 - loss: 1.6532 - val_accuracy: 0.5693 - val_loss: 1.5096\n",
      "Epoch 10/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 8ms/step - accuracy: 0.5340 - loss: 1.6497 - val_accuracy: 0.5656 - val_loss: 1.5141\n",
      "Epoch 11/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 8ms/step - accuracy: 0.5347 - loss: 1.6429 - val_accuracy: 0.5711 - val_loss: 1.5014\n",
      "Epoch 12/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 8ms/step - accuracy: 0.5329 - loss: 1.6500 - val_accuracy: 0.5766 - val_loss: 1.4857\n",
      "Epoch 13/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 8ms/step - accuracy: 0.5332 - loss: 1.6501 - val_accuracy: 0.5650 - val_loss: 1.5145\n",
      "Epoch 14/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 8ms/step - accuracy: 0.5349 - loss: 1.6392 - val_accuracy: 0.5638 - val_loss: 1.5130\n",
      "Epoch 15/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 8ms/step - accuracy: 0.5353 - loss: 1.6364 - val_accuracy: 0.5679 - val_loss: 1.5075\n",
      "Epoch 16/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 8ms/step - accuracy: 0.5353 - loss: 1.6350 - val_accuracy: 0.5739 - val_loss: 1.4856\n",
      "Epoch 17/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 8ms/step - accuracy: 0.5352 - loss: 1.6345 - val_accuracy: 0.5696 - val_loss: 1.4929\n",
      "Epoch 18/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 8ms/step - accuracy: 0.5338 - loss: 1.6414 - val_accuracy: 0.5726 - val_loss: 1.4911\n",
      "Epoch 19/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 7ms/step - accuracy: 0.5344 - loss: 1.6313 - val_accuracy: 0.5797 - val_loss: 1.4742\n",
      "Epoch 20/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 8ms/step - accuracy: 0.5357 - loss: 1.6370 - val_accuracy: 0.5770 - val_loss: 1.4769\n",
      "Epoch 21/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 8ms/step - accuracy: 0.5368 - loss: 1.6297 - val_accuracy: 0.5758 - val_loss: 1.4757\n",
      "Epoch 22/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 7ms/step - accuracy: 0.5368 - loss: 1.6292 - val_accuracy: 0.5692 - val_loss: 1.4938\n",
      "Epoch 23/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 7ms/step - accuracy: 0.5359 - loss: 1.6229 - val_accuracy: 0.5810 - val_loss: 1.4629\n",
      "Epoch 24/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 7ms/step - accuracy: 0.5382 - loss: 1.6266 - val_accuracy: 0.5808 - val_loss: 1.4675\n",
      "Epoch 25/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 7ms/step - accuracy: 0.5359 - loss: 1.6300 - val_accuracy: 0.5736 - val_loss: 1.4858\n",
      "Epoch 26/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 7ms/step - accuracy: 0.5345 - loss: 1.6365 - val_accuracy: 0.5722 - val_loss: 1.4900\n",
      "Epoch 27/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 7ms/step - accuracy: 0.5374 - loss: 1.6230 - val_accuracy: 0.5758 - val_loss: 1.4802\n",
      "Epoch 28/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 7ms/step - accuracy: 0.5363 - loss: 1.6335 - val_accuracy: 0.5817 - val_loss: 1.4622\n",
      "Epoch 29/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 8ms/step - accuracy: 0.5381 - loss: 1.6229 - val_accuracy: 0.5700 - val_loss: 1.4927\n",
      "Epoch 30/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 8ms/step - accuracy: 0.5391 - loss: 1.6226 - val_accuracy: 0.5783 - val_loss: 1.4738\n",
      "Epoch 31/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 8ms/step - accuracy: 0.5377 - loss: 1.6222 - val_accuracy: 0.5755 - val_loss: 1.4812\n",
      "Epoch 32/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 8ms/step - accuracy: 0.5386 - loss: 1.6223 - val_accuracy: 0.5746 - val_loss: 1.4837\n",
      "Epoch 33/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 8ms/step - accuracy: 0.5374 - loss: 1.6297 - val_accuracy: 0.5804 - val_loss: 1.4644\n",
      "Epoch 34/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 8ms/step - accuracy: 0.5373 - loss: 1.6283 - val_accuracy: 0.5810 - val_loss: 1.4595\n",
      "Epoch 35/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 8ms/step - accuracy: 0.5383 - loss: 1.6205 - val_accuracy: 0.5724 - val_loss: 1.4838\n",
      "Epoch 36/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 8ms/step - accuracy: 0.5383 - loss: 1.6201 - val_accuracy: 0.5662 - val_loss: 1.5035\n",
      "Epoch 37/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 8ms/step - accuracy: 0.5393 - loss: 1.6178 - val_accuracy: 0.5772 - val_loss: 1.4752\n",
      "Epoch 38/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 8ms/step - accuracy: 0.5386 - loss: 1.6191 - val_accuracy: 0.5737 - val_loss: 1.4809\n",
      "Epoch 39/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 8ms/step - accuracy: 0.5380 - loss: 1.6221 - val_accuracy: 0.5778 - val_loss: 1.4734\n",
      "Epoch 40/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 8ms/step - accuracy: 0.5398 - loss: 1.6224 - val_accuracy: 0.5756 - val_loss: 1.4792\n",
      "Epoch 41/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 8ms/step - accuracy: 0.5378 - loss: 1.6216 - val_accuracy: 0.5772 - val_loss: 1.4754\n",
      "Epoch 42/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 8ms/step - accuracy: 0.5387 - loss: 1.6241 - val_accuracy: 0.5782 - val_loss: 1.4632\n",
      "Epoch 43/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 8ms/step - accuracy: 0.5394 - loss: 1.6191 - val_accuracy: 0.5801 - val_loss: 1.4638\n",
      "Epoch 44/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 8ms/step - accuracy: 0.5366 - loss: 1.6282 - val_accuracy: 0.5721 - val_loss: 1.4838\n",
      "::::: model_lstm_16_16_seq_5 :::::\n",
      "\u001b[1m4341/4341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.6225 - loss: 1.3231\n",
      "[1.3911726474761963, 0.636749804019928]\n",
      "\u001b[1m4341/4341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A7       0.46      0.61      0.53      1177\n",
      "       AMaj7       0.69      0.63      0.66      4239\n",
      "       AMin7       0.66      0.67      0.66      6176\n",
      "         Ab7       0.32      0.70      0.44      1232\n",
      "      AbMaj7       0.66      0.65      0.65      4124\n",
      "      AbMin7       0.72      0.57      0.63      6631\n",
      "          B7       0.36      0.62      0.46      1163\n",
      "       BMaj7       0.74      0.59      0.66      3871\n",
      "       BMin7       0.70      0.59      0.64      6192\n",
      "         Bb7       0.35      0.63      0.45      1222\n",
      "      BbMaj7       0.67      0.71      0.69      3996\n",
      "      BbMin7       0.65      0.66      0.66      6313\n",
      "          C7       0.45      0.60      0.52      1193\n",
      "       CMaj7       0.71      0.65      0.68      3992\n",
      "       CMin7       0.75      0.62      0.68      6636\n",
      "          D7       0.50      0.59      0.54      1214\n",
      "       DMaj7       0.69      0.55      0.61      3896\n",
      "       DMin7       0.69      0.66      0.68      6272\n",
      "         Db7       0.37      0.62      0.47      1162\n",
      "      DbMaj7       0.68      0.63      0.66      4026\n",
      "      DbMin7       0.71      0.62      0.66      6576\n",
      "          E7       0.37      0.62      0.46      1176\n",
      "       EMaj7       0.62      0.65      0.64      4022\n",
      "       EMin7       0.69      0.63      0.66      6239\n",
      "         Eb7       0.36      0.62      0.46      1202\n",
      "      EbMaj7       0.61      0.74      0.67      3988\n",
      "      EbMin7       0.64      0.69      0.66      6240\n",
      "          F7       0.42      0.61      0.50      1165\n",
      "       FMaj7       0.62      0.67      0.65      3867\n",
      "       FMin7       0.67      0.65      0.66      6583\n",
      "          G7       0.46      0.67      0.55      1319\n",
      "       GMaj7       0.67      0.70      0.68      3851\n",
      "       GMin7       0.77      0.56      0.65      6224\n",
      "         Gb7       0.41      0.62      0.50      1150\n",
      "      GbMaj7       0.71      0.60      0.65      4003\n",
      "      GbMin7       0.70      0.67      0.68      6564\n",
      "\n",
      "    accuracy                           0.64    138896\n",
      "   macro avg       0.59      0.63      0.60    138896\n",
      "weighted avg       0.66      0.64      0.64    138896\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 8ms/step - accuracy: 0.4611 - loss: 2.0833 - val_accuracy: 0.5647 - val_loss: 1.5762\n",
      "Epoch 2/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 8ms/step - accuracy: 0.5433 - loss: 1.6444 - val_accuracy: 0.5802 - val_loss: 1.5085\n",
      "Epoch 3/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 8ms/step - accuracy: 0.5542 - loss: 1.5894 - val_accuracy: 0.5699 - val_loss: 1.5288\n",
      "Epoch 4/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 8ms/step - accuracy: 0.5581 - loss: 1.5676 - val_accuracy: 0.5853 - val_loss: 1.4728\n",
      "Epoch 5/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 8ms/step - accuracy: 0.5607 - loss: 1.5489 - val_accuracy: 0.5904 - val_loss: 1.4577\n",
      "Epoch 6/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 8ms/step - accuracy: 0.5629 - loss: 1.5399 - val_accuracy: 0.6002 - val_loss: 1.4155\n",
      "Epoch 7/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 8ms/step - accuracy: 0.5643 - loss: 1.5291 - val_accuracy: 0.5936 - val_loss: 1.4393\n",
      "Epoch 8/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 9ms/step - accuracy: 0.5654 - loss: 1.5282 - val_accuracy: 0.5997 - val_loss: 1.4249\n",
      "Epoch 9/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 8ms/step - accuracy: 0.5679 - loss: 1.5184 - val_accuracy: 0.6064 - val_loss: 1.4057\n",
      "Epoch 10/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 8ms/step - accuracy: 0.5683 - loss: 1.5160 - val_accuracy: 0.5995 - val_loss: 1.4197\n",
      "Epoch 11/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 8ms/step - accuracy: 0.5711 - loss: 1.5048 - val_accuracy: 0.6055 - val_loss: 1.4013\n",
      "Epoch 12/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 8ms/step - accuracy: 0.5694 - loss: 1.5105 - val_accuracy: 0.6097 - val_loss: 1.3940\n",
      "Epoch 13/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 8ms/step - accuracy: 0.5715 - loss: 1.5040 - val_accuracy: 0.6030 - val_loss: 1.4083\n",
      "Epoch 14/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 8ms/step - accuracy: 0.5721 - loss: 1.4971 - val_accuracy: 0.6033 - val_loss: 1.4056\n",
      "Epoch 15/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 8ms/step - accuracy: 0.5714 - loss: 1.4937 - val_accuracy: 0.6054 - val_loss: 1.3997\n",
      "Epoch 16/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 8ms/step - accuracy: 0.5721 - loss: 1.5046 - val_accuracy: 0.6034 - val_loss: 1.4055\n",
      "Epoch 17/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 9ms/step - accuracy: 0.5711 - loss: 1.5005 - val_accuracy: 0.6017 - val_loss: 1.4094\n",
      "Epoch 18/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 9ms/step - accuracy: 0.5714 - loss: 1.4972 - val_accuracy: 0.6022 - val_loss: 1.4055\n",
      "Epoch 19/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 9ms/step - accuracy: 0.5706 - loss: 1.4997 - val_accuracy: 0.6065 - val_loss: 1.3933\n",
      "Epoch 20/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 9ms/step - accuracy: 0.5730 - loss: 1.4924 - val_accuracy: 0.6046 - val_loss: 1.4002\n",
      "Epoch 21/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 9ms/step - accuracy: 0.5730 - loss: 1.4919 - val_accuracy: 0.6085 - val_loss: 1.3903\n",
      "Epoch 22/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 9ms/step - accuracy: 0.5726 - loss: 1.4923 - val_accuracy: 0.6034 - val_loss: 1.4062\n",
      "Epoch 23/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 9ms/step - accuracy: 0.5714 - loss: 1.4960 - val_accuracy: 0.6088 - val_loss: 1.3915\n",
      "Epoch 24/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 9ms/step - accuracy: 0.5723 - loss: 1.4975 - val_accuracy: 0.6117 - val_loss: 1.3809\n",
      "Epoch 25/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 9ms/step - accuracy: 0.5732 - loss: 1.4892 - val_accuracy: 0.6092 - val_loss: 1.3870\n",
      "Epoch 26/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 9ms/step - accuracy: 0.5745 - loss: 1.4854 - val_accuracy: 0.6059 - val_loss: 1.3916\n",
      "Epoch 27/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 9ms/step - accuracy: 0.5738 - loss: 1.4864 - val_accuracy: 0.6047 - val_loss: 1.3925\n",
      "Epoch 28/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 9ms/step - accuracy: 0.5729 - loss: 1.4918 - val_accuracy: 0.6071 - val_loss: 1.3918\n",
      "Epoch 29/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 9ms/step - accuracy: 0.5735 - loss: 1.4851 - val_accuracy: 0.6094 - val_loss: 1.3799\n",
      "Epoch 30/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 9ms/step - accuracy: 0.5730 - loss: 1.4901 - val_accuracy: 0.6148 - val_loss: 1.3692\n",
      "Epoch 31/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 9ms/step - accuracy: 0.5742 - loss: 1.4882 - val_accuracy: 0.6111 - val_loss: 1.3692\n",
      "Epoch 32/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 9ms/step - accuracy: 0.5769 - loss: 1.4797 - val_accuracy: 0.6052 - val_loss: 1.3848\n",
      "Epoch 33/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 9ms/step - accuracy: 0.5730 - loss: 1.4853 - val_accuracy: 0.6109 - val_loss: 1.3782\n",
      "Epoch 34/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 9ms/step - accuracy: 0.5754 - loss: 1.4817 - val_accuracy: 0.6129 - val_loss: 1.3756\n",
      "Epoch 35/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 9ms/step - accuracy: 0.5757 - loss: 1.4830 - val_accuracy: 0.6106 - val_loss: 1.3871\n",
      "Epoch 36/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 9ms/step - accuracy: 0.5752 - loss: 1.4818 - val_accuracy: 0.6121 - val_loss: 1.3816\n",
      "Epoch 37/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 9ms/step - accuracy: 0.5747 - loss: 1.4797 - val_accuracy: 0.6082 - val_loss: 1.3842\n",
      "Epoch 38/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 7ms/step - accuracy: 0.5752 - loss: 1.4801 - val_accuracy: 0.6062 - val_loss: 1.3820\n",
      "Epoch 39/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 7ms/step - accuracy: 0.5753 - loss: 1.4804 - val_accuracy: 0.6201 - val_loss: 1.3502\n",
      "Epoch 40/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 7ms/step - accuracy: 0.5747 - loss: 1.4851 - val_accuracy: 0.6138 - val_loss: 1.3652\n",
      "Epoch 41/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 7ms/step - accuracy: 0.5770 - loss: 1.4757 - val_accuracy: 0.6120 - val_loss: 1.3710\n",
      "Epoch 42/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 7ms/step - accuracy: 0.5736 - loss: 1.4807 - val_accuracy: 0.6146 - val_loss: 1.3679\n",
      "Epoch 43/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 7ms/step - accuracy: 0.5753 - loss: 1.4814 - val_accuracy: 0.6108 - val_loss: 1.3821\n",
      "Epoch 44/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 7ms/step - accuracy: 0.5759 - loss: 1.4768 - val_accuracy: 0.6167 - val_loss: 1.3648\n",
      "Epoch 45/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 7ms/step - accuracy: 0.5765 - loss: 1.4758 - val_accuracy: 0.6116 - val_loss: 1.3735\n",
      "Epoch 46/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 7ms/step - accuracy: 0.5769 - loss: 1.4775 - val_accuracy: 0.6163 - val_loss: 1.3649\n",
      "Epoch 47/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 7ms/step - accuracy: 0.5749 - loss: 1.4795 - val_accuracy: 0.6166 - val_loss: 1.3552\n",
      "Epoch 48/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 7ms/step - accuracy: 0.5769 - loss: 1.4744 - val_accuracy: 0.6068 - val_loss: 1.3916\n",
      "Epoch 49/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 7ms/step - accuracy: 0.5742 - loss: 1.4795 - val_accuracy: 0.6028 - val_loss: 1.3995\n",
      "::::: model_lstm_32_32_seq_5 :::::\n",
      "\u001b[1m4341/4341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.6512 - loss: 1.2502\n",
      "[1.2771985530853271, 0.6706744432449341]\n",
      "\u001b[1m4341/4341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A7       0.51      0.64      0.57      1177\n",
      "       AMaj7       0.75      0.67      0.71      4239\n",
      "       AMin7       0.70      0.68      0.69      6176\n",
      "         Ab7       0.36      0.71      0.47      1232\n",
      "      AbMaj7       0.70      0.63      0.67      4124\n",
      "      AbMin7       0.71      0.63      0.67      6631\n",
      "          B7       0.38      0.70      0.49      1163\n",
      "       BMaj7       0.70      0.68      0.69      3871\n",
      "       BMin7       0.76      0.61      0.68      6192\n",
      "         Bb7       0.55      0.63      0.59      1222\n",
      "      BbMaj7       0.72      0.69      0.71      3996\n",
      "      BbMin7       0.73      0.66      0.69      6313\n",
      "          C7       0.47      0.68      0.55      1193\n",
      "       CMaj7       0.71      0.67      0.69      3992\n",
      "       CMin7       0.73      0.67      0.70      6636\n",
      "          D7       0.43      0.69      0.53      1214\n",
      "       DMaj7       0.62      0.70      0.66      3896\n",
      "       DMin7       0.67      0.71      0.69      6272\n",
      "         Db7       0.61      0.61      0.61      1162\n",
      "      DbMaj7       0.71      0.65      0.68      4026\n",
      "      DbMin7       0.75      0.65      0.70      6576\n",
      "          E7       0.44      0.68      0.54      1176\n",
      "       EMaj7       0.61      0.72      0.66      4022\n",
      "       EMin7       0.71      0.67      0.69      6239\n",
      "         Eb7       0.40      0.69      0.51      1202\n",
      "      EbMaj7       0.66      0.73      0.70      3988\n",
      "      EbMin7       0.70      0.70      0.70      6240\n",
      "          F7       0.40      0.68      0.51      1165\n",
      "       FMaj7       0.71      0.64      0.67      3867\n",
      "       FMin7       0.72      0.67      0.69      6583\n",
      "          G7       0.60      0.72      0.65      1319\n",
      "       GMaj7       0.76      0.65      0.70      3851\n",
      "       GMin7       0.74      0.67      0.70      6224\n",
      "         Gb7       0.48      0.70      0.57      1150\n",
      "      GbMaj7       0.71      0.68      0.69      4003\n",
      "      GbMin7       0.74      0.67      0.70      6564\n",
      "\n",
      "    accuracy                           0.67    138896\n",
      "   macro avg       0.63      0.67      0.64    138896\n",
      "weighted avg       0.69      0.67      0.67    138896\n",
      "\n",
      "\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 8ms/step - accuracy: 0.4940 - loss: 1.9867 - val_accuracy: 0.5722 - val_loss: 1.5718\n",
      "Epoch 2/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 8ms/step - accuracy: 0.5641 - loss: 1.5940 - val_accuracy: 0.5944 - val_loss: 1.4926\n",
      "Epoch 3/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 8ms/step - accuracy: 0.5746 - loss: 1.5418 - val_accuracy: 0.6087 - val_loss: 1.4280\n",
      "Epoch 4/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 8ms/step - accuracy: 0.5820 - loss: 1.5050 - val_accuracy: 0.6134 - val_loss: 1.4260\n",
      "Epoch 5/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 8ms/step - accuracy: 0.5863 - loss: 1.4859 - val_accuracy: 0.6159 - val_loss: 1.4112\n",
      "Epoch 6/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 8ms/step - accuracy: 0.5890 - loss: 1.4658 - val_accuracy: 0.6222 - val_loss: 1.3964\n",
      "Epoch 7/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 8ms/step - accuracy: 0.5908 - loss: 1.4583 - val_accuracy: 0.6184 - val_loss: 1.3969\n",
      "Epoch 8/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 8ms/step - accuracy: 0.5924 - loss: 1.4515 - val_accuracy: 0.6273 - val_loss: 1.3754\n",
      "Epoch 9/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 8ms/step - accuracy: 0.5932 - loss: 1.4471 - val_accuracy: 0.6285 - val_loss: 1.3594\n",
      "Epoch 10/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 8ms/step - accuracy: 0.5935 - loss: 1.4419 - val_accuracy: 0.6308 - val_loss: 1.3596\n",
      "Epoch 11/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 8ms/step - accuracy: 0.5978 - loss: 1.4330 - val_accuracy: 0.6256 - val_loss: 1.3696\n",
      "Epoch 12/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 8ms/step - accuracy: 0.5955 - loss: 1.4334 - val_accuracy: 0.6339 - val_loss: 1.3487\n",
      "Epoch 13/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 8ms/step - accuracy: 0.5966 - loss: 1.4319 - val_accuracy: 0.6308 - val_loss: 1.3533\n",
      "Epoch 14/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 8ms/step - accuracy: 0.5991 - loss: 1.4236 - val_accuracy: 0.6306 - val_loss: 1.3644\n",
      "Epoch 15/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 8ms/step - accuracy: 0.5995 - loss: 1.4275 - val_accuracy: 0.6380 - val_loss: 1.3359\n",
      "Epoch 16/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 9ms/step - accuracy: 0.5987 - loss: 1.4245 - val_accuracy: 0.6327 - val_loss: 1.3495\n",
      "Epoch 17/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 9ms/step - accuracy: 0.6005 - loss: 1.4178 - val_accuracy: 0.6337 - val_loss: 1.3514\n",
      "Epoch 18/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 9ms/step - accuracy: 0.5999 - loss: 1.4117 - val_accuracy: 0.6424 - val_loss: 1.3241\n",
      "Epoch 19/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 9ms/step - accuracy: 0.6025 - loss: 1.4134 - val_accuracy: 0.6319 - val_loss: 1.3505\n",
      "Epoch 20/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 9ms/step - accuracy: 0.6003 - loss: 1.4202 - val_accuracy: 0.6356 - val_loss: 1.3297\n",
      "Epoch 21/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 9ms/step - accuracy: 0.5999 - loss: 1.4143 - val_accuracy: 0.6378 - val_loss: 1.3354\n",
      "Epoch 22/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 9ms/step - accuracy: 0.6029 - loss: 1.4082 - val_accuracy: 0.6353 - val_loss: 1.3369\n",
      "Epoch 23/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 9ms/step - accuracy: 0.6009 - loss: 1.4050 - val_accuracy: 0.6382 - val_loss: 1.3315\n",
      "Epoch 24/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 9ms/step - accuracy: 0.6023 - loss: 1.4078 - val_accuracy: 0.6400 - val_loss: 1.3315\n",
      "Epoch 25/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 9ms/step - accuracy: 0.6022 - loss: 1.4096 - val_accuracy: 0.6320 - val_loss: 1.3584\n",
      "Epoch 26/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 9ms/step - accuracy: 0.6015 - loss: 1.4103 - val_accuracy: 0.6305 - val_loss: 1.3563\n",
      "Epoch 27/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 9ms/step - accuracy: 0.6017 - loss: 1.4056 - val_accuracy: 0.6327 - val_loss: 1.3453\n",
      "Epoch 28/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 9ms/step - accuracy: 0.6037 - loss: 1.4000 - val_accuracy: 0.6404 - val_loss: 1.3267\n",
      "::::: model_lstm_64_64_seq_5 :::::\n",
      "\u001b[1m4341/4341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.6602 - loss: 1.2566\n",
      "[1.275354266166687, 0.6773917078971863]\n",
      "\u001b[1m4341/4341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A7       0.43      0.75      0.55      1177\n",
      "       AMaj7       0.69      0.72      0.70      4239\n",
      "       AMin7       0.73      0.65      0.69      6176\n",
      "         Ab7       0.57      0.66      0.61      1232\n",
      "      AbMaj7       0.72      0.64      0.68      4124\n",
      "      AbMin7       0.77      0.63      0.70      6631\n",
      "          B7       0.40      0.76      0.53      1163\n",
      "       BMaj7       0.69      0.73      0.71      3871\n",
      "       BMin7       0.74      0.65      0.70      6192\n",
      "         Bb7       0.41      0.71      0.52      1222\n",
      "      BbMaj7       0.69      0.72      0.70      3996\n",
      "      BbMin7       0.74      0.65      0.69      6313\n",
      "          C7       0.55      0.70      0.61      1193\n",
      "       CMaj7       0.70      0.69      0.70      3992\n",
      "       CMin7       0.77      0.64      0.70      6636\n",
      "          D7       0.39      0.74      0.51      1214\n",
      "       DMaj7       0.69      0.67      0.68      3896\n",
      "       DMin7       0.72      0.70      0.71      6272\n",
      "         Db7       0.46      0.78      0.58      1162\n",
      "      DbMaj7       0.68      0.70      0.69      4026\n",
      "      DbMin7       0.72      0.71      0.72      6576\n",
      "          E7       0.46      0.78      0.58      1176\n",
      "       EMaj7       0.68      0.71      0.70      4022\n",
      "       EMin7       0.81      0.60      0.69      6239\n",
      "         Eb7       0.54      0.70      0.61      1202\n",
      "      EbMaj7       0.65      0.72      0.68      3988\n",
      "      EbMin7       0.71      0.68      0.70      6240\n",
      "          F7       0.41      0.77      0.53      1165\n",
      "       FMaj7       0.72      0.66      0.69      3867\n",
      "       FMin7       0.69      0.69      0.69      6583\n",
      "          G7       0.38      0.83      0.52      1319\n",
      "       GMaj7       0.73      0.71      0.72      3851\n",
      "       GMin7       0.78      0.54      0.64      6224\n",
      "         Gb7       0.56      0.66      0.60      1150\n",
      "      GbMaj7       0.73      0.69      0.71      4003\n",
      "      GbMin7       0.75      0.68      0.72      6564\n",
      "\n",
      "    accuracy                           0.68    138896\n",
      "   macro avg       0.64      0.70      0.65    138896\n",
      "weighted avg       0.70      0.68      0.68    138896\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "2025-07-31 00:32:56.767213: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 184507680 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 10ms/step - accuracy: 0.3987 - loss: 2.2921 - val_accuracy: 0.5438 - val_loss: 1.6263\n",
      "Epoch 2/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 10ms/step - accuracy: 0.5113 - loss: 1.7538 - val_accuracy: 0.5472 - val_loss: 1.5911\n",
      "Epoch 3/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 10ms/step - accuracy: 0.5187 - loss: 1.7105 - val_accuracy: 0.5558 - val_loss: 1.5621\n",
      "Epoch 4/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 10ms/step - accuracy: 0.5252 - loss: 1.6844 - val_accuracy: 0.5591 - val_loss: 1.5482\n",
      "Epoch 5/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 10ms/step - accuracy: 0.5266 - loss: 1.6723 - val_accuracy: 0.5593 - val_loss: 1.5396\n",
      "Epoch 6/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 10ms/step - accuracy: 0.5304 - loss: 1.6584 - val_accuracy: 0.5617 - val_loss: 1.5263\n",
      "Epoch 7/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 10ms/step - accuracy: 0.5305 - loss: 1.6560 - val_accuracy: 0.5662 - val_loss: 1.5192\n",
      "Epoch 8/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 10ms/step - accuracy: 0.5318 - loss: 1.6480 - val_accuracy: 0.5741 - val_loss: 1.4923\n",
      "Epoch 9/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 10ms/step - accuracy: 0.5335 - loss: 1.6412 - val_accuracy: 0.5720 - val_loss: 1.4934\n",
      "Epoch 10/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 10ms/step - accuracy: 0.5327 - loss: 1.6360 - val_accuracy: 0.5713 - val_loss: 1.4922\n",
      "Epoch 11/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 10ms/step - accuracy: 0.5343 - loss: 1.6394 - val_accuracy: 0.5632 - val_loss: 1.5110\n",
      "Epoch 12/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 10ms/step - accuracy: 0.5357 - loss: 1.6293 - val_accuracy: 0.5759 - val_loss: 1.4829\n",
      "Epoch 13/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 10ms/step - accuracy: 0.5362 - loss: 1.6237 - val_accuracy: 0.5737 - val_loss: 1.4876\n",
      "Epoch 14/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 10ms/step - accuracy: 0.5380 - loss: 1.6217 - val_accuracy: 0.5708 - val_loss: 1.4983\n",
      "Epoch 15/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 10ms/step - accuracy: 0.5358 - loss: 1.6206 - val_accuracy: 0.5739 - val_loss: 1.4893\n",
      "Epoch 16/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 10ms/step - accuracy: 0.5371 - loss: 1.6279 - val_accuracy: 0.5666 - val_loss: 1.5023\n",
      "Epoch 17/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 10ms/step - accuracy: 0.5362 - loss: 1.6192 - val_accuracy: 0.5657 - val_loss: 1.5084\n",
      "Epoch 18/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 10ms/step - accuracy: 0.5380 - loss: 1.6192 - val_accuracy: 0.5747 - val_loss: 1.4782\n",
      "Epoch 19/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 10ms/step - accuracy: 0.5386 - loss: 1.6219 - val_accuracy: 0.5766 - val_loss: 1.4750\n",
      "Epoch 20/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 10ms/step - accuracy: 0.5395 - loss: 1.6187 - val_accuracy: 0.5741 - val_loss: 1.4781\n",
      "Epoch 21/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 10ms/step - accuracy: 0.5394 - loss: 1.6191 - val_accuracy: 0.5669 - val_loss: 1.5009\n",
      "Epoch 22/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 10ms/step - accuracy: 0.5381 - loss: 1.6172 - val_accuracy: 0.5635 - val_loss: 1.5021\n",
      "Epoch 23/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 10ms/step - accuracy: 0.5380 - loss: 1.6153 - val_accuracy: 0.5796 - val_loss: 1.4561\n",
      "Epoch 24/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 10ms/step - accuracy: 0.5400 - loss: 1.6114 - val_accuracy: 0.5766 - val_loss: 1.4737\n",
      "Epoch 25/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 10ms/step - accuracy: 0.5421 - loss: 1.6072 - val_accuracy: 0.5794 - val_loss: 1.4665\n",
      "Epoch 26/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 10ms/step - accuracy: 0.5392 - loss: 1.6115 - val_accuracy: 0.5786 - val_loss: 1.4694\n",
      "Epoch 27/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 10ms/step - accuracy: 0.5402 - loss: 1.6115 - val_accuracy: 0.5767 - val_loss: 1.4761\n",
      "Epoch 28/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 10ms/step - accuracy: 0.5408 - loss: 1.6125 - val_accuracy: 0.5753 - val_loss: 1.4786\n",
      "Epoch 29/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 10ms/step - accuracy: 0.5411 - loss: 1.6065 - val_accuracy: 0.5795 - val_loss: 1.4675\n",
      "Epoch 30/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 10ms/step - accuracy: 0.5416 - loss: 1.6074 - val_accuracy: 0.5702 - val_loss: 1.4871\n",
      "Epoch 31/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 10ms/step - accuracy: 0.5404 - loss: 1.6117 - val_accuracy: 0.5803 - val_loss: 1.4651\n",
      "Epoch 32/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 10ms/step - accuracy: 0.5413 - loss: 1.6090 - val_accuracy: 0.5776 - val_loss: 1.4723\n",
      "Epoch 33/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 10ms/step - accuracy: 0.5401 - loss: 1.6101 - val_accuracy: 0.5764 - val_loss: 1.4736\n",
      "::::: model_lstm_16_16_seq_10 :::::\n",
      "\u001b[1m4341/4341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.6249 - loss: 1.3309\n",
      "[1.390663981437683, 0.6377015113830566]\n",
      "\u001b[1m4341/4341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A7       0.50      0.57      0.53      1177\n",
      "       AMaj7       0.72      0.65      0.68      4239\n",
      "       AMin7       0.72      0.55      0.62      6176\n",
      "         Ab7       0.36      0.62      0.46      1232\n",
      "      AbMaj7       0.71      0.60      0.65      4124\n",
      "      AbMin7       0.72      0.61      0.66      6631\n",
      "          B7       0.45      0.57      0.50      1163\n",
      "       BMaj7       0.66      0.65      0.66      3871\n",
      "       BMin7       0.71      0.63      0.67      6192\n",
      "         Bb7       0.45      0.60      0.51      1222\n",
      "      BbMaj7       0.66      0.65      0.66      3996\n",
      "      BbMin7       0.67      0.65      0.66      6313\n",
      "          C7       0.31      0.69      0.43      1193\n",
      "       CMaj7       0.55      0.79      0.65      3992\n",
      "       CMin7       0.74      0.60      0.66      6636\n",
      "          D7       0.35      0.65      0.46      1214\n",
      "       DMaj7       0.69      0.58      0.63      3896\n",
      "       DMin7       0.78      0.58      0.66      6272\n",
      "         Db7       0.50      0.62      0.55      1162\n",
      "      DbMaj7       0.70      0.63      0.66      4026\n",
      "      DbMin7       0.66      0.69      0.67      6576\n",
      "          E7       0.47      0.59      0.52      1176\n",
      "       EMaj7       0.63      0.66      0.65      4022\n",
      "       EMin7       0.75      0.60      0.66      6239\n",
      "         Eb7       0.44      0.59      0.50      1202\n",
      "      EbMaj7       0.66      0.67      0.66      3988\n",
      "      EbMin7       0.69      0.64      0.67      6240\n",
      "          F7       0.37      0.63      0.46      1165\n",
      "       FMaj7       0.60      0.70      0.64      3867\n",
      "       FMin7       0.68      0.62      0.64      6578\n",
      "          G7       0.42      0.68      0.52      1319\n",
      "       GMaj7       0.67      0.70      0.68      3851\n",
      "       GMin7       0.64      0.67      0.65      6224\n",
      "         Gb7       0.39      0.62      0.48      1150\n",
      "      GbMaj7       0.65      0.69      0.67      4003\n",
      "      GbMin7       0.72      0.66      0.69      6564\n",
      "\n",
      "    accuracy                           0.64    138891\n",
      "   macro avg       0.59      0.64      0.60    138891\n",
      "weighted avg       0.66      0.64      0.64    138891\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "2025-07-31 01:41:22.715613: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 184507680 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 11ms/step - accuracy: 0.4518 - loss: 2.1135 - val_accuracy: 0.5669 - val_loss: 1.5562\n",
      "Epoch 2/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 11ms/step - accuracy: 0.5412 - loss: 1.6353 - val_accuracy: 0.5719 - val_loss: 1.5109\n",
      "Epoch 3/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 11ms/step - accuracy: 0.5566 - loss: 1.5692 - val_accuracy: 0.5854 - val_loss: 1.4744\n",
      "Epoch 4/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 11ms/step - accuracy: 0.5598 - loss: 1.5520 - val_accuracy: 0.5960 - val_loss: 1.4398\n",
      "Epoch 5/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 11ms/step - accuracy: 0.5642 - loss: 1.5320 - val_accuracy: 0.5888 - val_loss: 1.4530\n",
      "Epoch 6/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 11ms/step - accuracy: 0.5678 - loss: 1.5142 - val_accuracy: 0.5970 - val_loss: 1.4260\n",
      "Epoch 7/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 11ms/step - accuracy: 0.5686 - loss: 1.5125 - val_accuracy: 0.5976 - val_loss: 1.4216\n",
      "Epoch 8/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 11ms/step - accuracy: 0.5725 - loss: 1.5062 - val_accuracy: 0.5965 - val_loss: 1.4308\n",
      "Epoch 9/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 11ms/step - accuracy: 0.5732 - loss: 1.5027 - val_accuracy: 0.6081 - val_loss: 1.3942\n",
      "Epoch 10/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 11ms/step - accuracy: 0.5757 - loss: 1.4976 - val_accuracy: 0.5989 - val_loss: 1.4174\n",
      "Epoch 11/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 11ms/step - accuracy: 0.5760 - loss: 1.4888 - val_accuracy: 0.6002 - val_loss: 1.4182\n",
      "Epoch 12/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 11ms/step - accuracy: 0.5740 - loss: 1.4952 - val_accuracy: 0.6039 - val_loss: 1.4052\n",
      "Epoch 13/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 11ms/step - accuracy: 0.5752 - loss: 1.4961 - val_accuracy: 0.5987 - val_loss: 1.4286\n",
      "Epoch 14/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 11ms/step - accuracy: 0.5758 - loss: 1.4913 - val_accuracy: 0.6033 - val_loss: 1.4000\n",
      "Epoch 15/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 11ms/step - accuracy: 0.5751 - loss: 1.4872 - val_accuracy: 0.6069 - val_loss: 1.3987\n",
      "Epoch 16/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 11ms/step - accuracy: 0.5751 - loss: 1.4823 - val_accuracy: 0.6026 - val_loss: 1.4027\n",
      "Epoch 17/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 11ms/step - accuracy: 0.5760 - loss: 1.4842 - val_accuracy: 0.6067 - val_loss: 1.3967\n",
      "Epoch 18/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 11ms/step - accuracy: 0.5759 - loss: 1.4837 - val_accuracy: 0.6050 - val_loss: 1.4015\n",
      "Epoch 19/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 11ms/step - accuracy: 0.5770 - loss: 1.4822 - val_accuracy: 0.6101 - val_loss: 1.3849\n",
      "Epoch 20/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 11ms/step - accuracy: 0.5795 - loss: 1.4706 - val_accuracy: 0.6090 - val_loss: 1.3886\n",
      "Epoch 21/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 11ms/step - accuracy: 0.5766 - loss: 1.4771 - val_accuracy: 0.6076 - val_loss: 1.3996\n",
      "Epoch 22/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 11ms/step - accuracy: 0.5771 - loss: 1.4736 - val_accuracy: 0.6096 - val_loss: 1.3935\n",
      "Epoch 23/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 11ms/step - accuracy: 0.5767 - loss: 1.4797 - val_accuracy: 0.6111 - val_loss: 1.3842\n",
      "Epoch 24/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 11ms/step - accuracy: 0.5799 - loss: 1.4703 - val_accuracy: 0.6103 - val_loss: 1.3902\n",
      "Epoch 25/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 11ms/step - accuracy: 0.5787 - loss: 1.4725 - val_accuracy: 0.6105 - val_loss: 1.3756\n",
      "Epoch 26/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 11ms/step - accuracy: 0.5790 - loss: 1.4712 - val_accuracy: 0.6054 - val_loss: 1.4021\n",
      "Epoch 27/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 11ms/step - accuracy: 0.5787 - loss: 1.4756 - val_accuracy: 0.6111 - val_loss: 1.3812\n",
      "Epoch 28/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 11ms/step - accuracy: 0.5804 - loss: 1.4706 - val_accuracy: 0.6088 - val_loss: 1.3952\n",
      "Epoch 29/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 11ms/step - accuracy: 0.5801 - loss: 1.4713 - val_accuracy: 0.6176 - val_loss: 1.3563\n",
      "Epoch 30/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 11ms/step - accuracy: 0.5798 - loss: 1.4674 - val_accuracy: 0.6076 - val_loss: 1.3953\n",
      "Epoch 31/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 11ms/step - accuracy: 0.5789 - loss: 1.4684 - val_accuracy: 0.6143 - val_loss: 1.3734\n",
      "Epoch 32/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 11ms/step - accuracy: 0.5785 - loss: 1.4750 - val_accuracy: 0.6137 - val_loss: 1.3742\n",
      "Epoch 33/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 11ms/step - accuracy: 0.5810 - loss: 1.4611 - val_accuracy: 0.6108 - val_loss: 1.3859\n",
      "Epoch 34/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 11ms/step - accuracy: 0.5801 - loss: 1.4625 - val_accuracy: 0.6080 - val_loss: 1.3917\n",
      "Epoch 35/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 11ms/step - accuracy: 0.5800 - loss: 1.4718 - val_accuracy: 0.6145 - val_loss: 1.3697\n",
      "Epoch 36/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 11ms/step - accuracy: 0.5794 - loss: 1.4667 - val_accuracy: 0.6129 - val_loss: 1.3719\n",
      "Epoch 37/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 11ms/step - accuracy: 0.5817 - loss: 1.4620 - val_accuracy: 0.6115 - val_loss: 1.3764\n",
      "Epoch 38/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 11ms/step - accuracy: 0.5781 - loss: 1.4757 - val_accuracy: 0.6154 - val_loss: 1.3665\n",
      "Epoch 39/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 11ms/step - accuracy: 0.5812 - loss: 1.4677 - val_accuracy: 0.6147 - val_loss: 1.3747\n",
      "::::: model_lstm_32_32_seq_10 :::::\n",
      "\u001b[1m4341/4341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.6448 - loss: 1.2754\n",
      "[1.3093490600585938, 0.660618782043457]\n",
      "\u001b[1m4341/4341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A7       0.41      0.70      0.52      1177\n",
      "       AMaj7       0.70      0.67      0.69      4239\n",
      "       AMin7       0.70      0.63      0.67      6176\n",
      "         Ab7       0.40      0.74      0.52      1232\n",
      "      AbMaj7       0.68      0.62      0.65      4124\n",
      "      AbMin7       0.71      0.66      0.68      6631\n",
      "          B7       0.48      0.67      0.56      1163\n",
      "       BMaj7       0.64      0.68      0.66      3871\n",
      "       BMin7       0.68      0.69      0.68      6192\n",
      "         Bb7       0.46      0.68      0.55      1222\n",
      "      BbMaj7       0.66      0.70      0.68      3996\n",
      "      BbMin7       0.72      0.66      0.69      6313\n",
      "          C7       0.49      0.69      0.57      1193\n",
      "       CMaj7       0.75      0.63      0.69      3992\n",
      "       CMin7       0.75      0.65      0.69      6636\n",
      "          D7       0.46      0.65      0.54      1214\n",
      "       DMaj7       0.68      0.63      0.65      3896\n",
      "       DMin7       0.73      0.68      0.70      6272\n",
      "         Db7       0.41      0.78      0.54      1162\n",
      "      DbMaj7       0.64      0.71      0.67      4026\n",
      "      DbMin7       0.74      0.65      0.69      6576\n",
      "          E7       0.43      0.68      0.53      1176\n",
      "       EMaj7       0.70      0.63      0.67      4022\n",
      "       EMin7       0.72      0.65      0.69      6239\n",
      "         Eb7       0.55      0.60      0.58      1202\n",
      "      EbMaj7       0.66      0.67      0.67      3988\n",
      "      EbMin7       0.70      0.69      0.69      6240\n",
      "          F7       0.35      0.73      0.48      1165\n",
      "       FMaj7       0.71      0.68      0.69      3867\n",
      "       FMin7       0.66      0.67      0.67      6578\n",
      "          G7       0.45      0.74      0.56      1319\n",
      "       GMaj7       0.72      0.63      0.67      3851\n",
      "       GMin7       0.77      0.58      0.66      6224\n",
      "         Gb7       0.45      0.67      0.54      1150\n",
      "      GbMaj7       0.75      0.63      0.68      4003\n",
      "      GbMin7       0.72      0.67      0.69      6564\n",
      "\n",
      "    accuracy                           0.66    138891\n",
      "   macro avg       0.62      0.67      0.63    138891\n",
      "weighted avg       0.68      0.66      0.67    138891\n",
      "\n",
      "\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "2025-07-31 03:07:13.117284: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 184507680 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 13ms/step - accuracy: 0.4839 - loss: 1.9951 - val_accuracy: 0.5839 - val_loss: 1.5317\n",
      "Epoch 2/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 13ms/step - accuracy: 0.5618 - loss: 1.5966 - val_accuracy: 0.5931 - val_loss: 1.5019\n",
      "Epoch 3/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 13ms/step - accuracy: 0.5790 - loss: 1.5241 - val_accuracy: 0.6117 - val_loss: 1.4279\n",
      "Epoch 4/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 13ms/step - accuracy: 0.5850 - loss: 1.5025 - val_accuracy: 0.6161 - val_loss: 1.4186\n",
      "Epoch 5/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 13ms/step - accuracy: 0.5917 - loss: 1.4736 - val_accuracy: 0.6116 - val_loss: 1.4287\n",
      "Epoch 6/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 14ms/step - accuracy: 0.5953 - loss: 1.4610 - val_accuracy: 0.6292 - val_loss: 1.3819\n",
      "Epoch 7/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 14ms/step - accuracy: 0.5952 - loss: 1.4615 - val_accuracy: 0.6294 - val_loss: 1.3787\n",
      "Epoch 8/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 14ms/step - accuracy: 0.5968 - loss: 1.4492 - val_accuracy: 0.6253 - val_loss: 1.3799\n",
      "Epoch 9/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 14ms/step - accuracy: 0.5975 - loss: 1.4396 - val_accuracy: 0.6325 - val_loss: 1.3620\n",
      "Epoch 10/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 14ms/step - accuracy: 0.5984 - loss: 1.4381 - val_accuracy: 0.6287 - val_loss: 1.3699\n",
      "Epoch 11/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 14ms/step - accuracy: 0.5992 - loss: 1.4350 - val_accuracy: 0.6355 - val_loss: 1.3562\n",
      "Epoch 12/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 14ms/step - accuracy: 0.5997 - loss: 1.4324 - val_accuracy: 0.6346 - val_loss: 1.3606\n",
      "Epoch 13/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 14ms/step - accuracy: 0.5980 - loss: 1.4325 - val_accuracy: 0.6407 - val_loss: 1.3429\n",
      "Epoch 14/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 14ms/step - accuracy: 0.6028 - loss: 1.4237 - val_accuracy: 0.6276 - val_loss: 1.3778\n",
      "Epoch 15/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 14ms/step - accuracy: 0.6030 - loss: 1.4224 - val_accuracy: 0.6385 - val_loss: 1.3464\n",
      "Epoch 16/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 14ms/step - accuracy: 0.5995 - loss: 1.4260 - val_accuracy: 0.6324 - val_loss: 1.3530\n",
      "Epoch 17/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 14ms/step - accuracy: 0.6039 - loss: 1.4175 - val_accuracy: 0.6391 - val_loss: 1.3462\n",
      "Epoch 18/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 14ms/step - accuracy: 0.6017 - loss: 1.4211 - val_accuracy: 0.6385 - val_loss: 1.3460\n",
      "Epoch 19/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 14ms/step - accuracy: 0.6026 - loss: 1.4186 - val_accuracy: 0.6318 - val_loss: 1.3677\n",
      "Epoch 20/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 14ms/step - accuracy: 0.6037 - loss: 1.4103 - val_accuracy: 0.6319 - val_loss: 1.3590\n",
      "Epoch 21/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 14ms/step - accuracy: 0.6039 - loss: 1.4108 - val_accuracy: 0.6263 - val_loss: 1.3727\n",
      "Epoch 22/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 14ms/step - accuracy: 0.6035 - loss: 1.4103 - val_accuracy: 0.6427 - val_loss: 1.3315\n",
      "Epoch 23/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 14ms/step - accuracy: 0.6048 - loss: 1.4047 - val_accuracy: 0.6308 - val_loss: 1.3565\n",
      "Epoch 24/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 14ms/step - accuracy: 0.6042 - loss: 1.4104 - val_accuracy: 0.6362 - val_loss: 1.3414\n",
      "Epoch 25/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 14ms/step - accuracy: 0.6058 - loss: 1.4031 - val_accuracy: 0.6375 - val_loss: 1.3330\n",
      "Epoch 26/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 14ms/step - accuracy: 0.6038 - loss: 1.4073 - val_accuracy: 0.6403 - val_loss: 1.3324\n",
      "Epoch 27/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 14ms/step - accuracy: 0.6049 - loss: 1.4065 - val_accuracy: 0.6354 - val_loss: 1.3445\n",
      "Epoch 28/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 14ms/step - accuracy: 0.6052 - loss: 1.4103 - val_accuracy: 0.6362 - val_loss: 1.3412\n",
      "Epoch 29/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 14ms/step - accuracy: 0.6047 - loss: 1.4091 - val_accuracy: 0.6303 - val_loss: 1.3665\n",
      "Epoch 30/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 14ms/step - accuracy: 0.6058 - loss: 1.4002 - val_accuracy: 0.6306 - val_loss: 1.3526\n",
      "Epoch 31/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 14ms/step - accuracy: 0.6044 - loss: 1.4049 - val_accuracy: 0.6310 - val_loss: 1.3525\n",
      "Epoch 32/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 14ms/step - accuracy: 0.6045 - loss: 1.4063 - val_accuracy: 0.6365 - val_loss: 1.3295\n",
      "Epoch 33/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 14ms/step - accuracy: 0.6050 - loss: 1.4000 - val_accuracy: 0.6351 - val_loss: 1.3358\n",
      "Epoch 34/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 14ms/step - accuracy: 0.6036 - loss: 1.4096 - val_accuracy: 0.6360 - val_loss: 1.3447\n",
      "Epoch 35/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 14ms/step - accuracy: 0.6034 - loss: 1.4060 - val_accuracy: 0.6397 - val_loss: 1.3342\n",
      "Epoch 36/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 15ms/step - accuracy: 0.6070 - loss: 1.3963 - val_accuracy: 0.6417 - val_loss: 1.3269\n",
      "Epoch 37/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 15ms/step - accuracy: 0.6062 - loss: 1.3986 - val_accuracy: 0.6391 - val_loss: 1.3319\n",
      "Epoch 38/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 15ms/step - accuracy: 0.6053 - loss: 1.3981 - val_accuracy: 0.6436 - val_loss: 1.3127\n",
      "Epoch 39/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 15ms/step - accuracy: 0.6054 - loss: 1.3952 - val_accuracy: 0.6367 - val_loss: 1.3319\n",
      "Epoch 40/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 15ms/step - accuracy: 0.6069 - loss: 1.3913 - val_accuracy: 0.6423 - val_loss: 1.3228\n",
      "Epoch 41/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 15ms/step - accuracy: 0.6048 - loss: 1.3997 - val_accuracy: 0.6426 - val_loss: 1.3242\n",
      "Epoch 42/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 15ms/step - accuracy: 0.6063 - loss: 1.3949 - val_accuracy: 0.6356 - val_loss: 1.3405\n",
      "Epoch 43/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 15ms/step - accuracy: 0.6061 - loss: 1.3958 - val_accuracy: 0.6449 - val_loss: 1.3165\n",
      "Epoch 44/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 15ms/step - accuracy: 0.6074 - loss: 1.3888 - val_accuracy: 0.6384 - val_loss: 1.3377\n",
      "Epoch 45/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 15ms/step - accuracy: 0.6061 - loss: 1.3936 - val_accuracy: 0.6446 - val_loss: 1.3169\n",
      "Epoch 46/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 15ms/step - accuracy: 0.6084 - loss: 1.3875 - val_accuracy: 0.6412 - val_loss: 1.3166\n",
      "Epoch 47/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 15ms/step - accuracy: 0.6079 - loss: 1.3903 - val_accuracy: 0.6412 - val_loss: 1.3231\n",
      "Epoch 48/500\n",
      "\u001b[1m12013/12013\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 15ms/step - accuracy: 0.6045 - loss: 1.3971 - val_accuracy: 0.6412 - val_loss: 1.3240\n",
      "::::: model_lstm_64_64_seq_10 :::::\n",
      "\u001b[1m4341/4341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 4ms/step - accuracy: 0.6703 - loss: 1.2535\n",
      "[1.2804495096206665, 0.6779704689979553]\n",
      "\u001b[1m4341/4341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A7       0.42      0.74      0.54      1177\n",
      "       AMaj7       0.74      0.61      0.67      4239\n",
      "       AMin7       0.72      0.64      0.68      6176\n",
      "         Ab7       0.50      0.73      0.59      1232\n",
      "      AbMaj7       0.71      0.67      0.69      4124\n",
      "      AbMin7       0.71      0.65      0.68      6631\n",
      "          B7       0.48      0.77      0.60      1163\n",
      "       BMaj7       0.68      0.64      0.66      3871\n",
      "       BMin7       0.81      0.62      0.70      6192\n",
      "         Bb7       0.49      0.79      0.61      1222\n",
      "      BbMaj7       0.72      0.69      0.71      3996\n",
      "      BbMin7       0.79      0.62      0.69      6313\n",
      "          C7       0.42      0.77      0.54      1193\n",
      "       CMaj7       0.70      0.69      0.69      3992\n",
      "       CMin7       0.71      0.70      0.71      6636\n",
      "          D7       0.52      0.75      0.62      1214\n",
      "       DMaj7       0.70      0.63      0.66      3896\n",
      "       DMin7       0.71      0.67      0.69      6272\n",
      "         Db7       0.49      0.78      0.60      1162\n",
      "      DbMaj7       0.68      0.71      0.69      4026\n",
      "      DbMin7       0.68      0.74      0.71      6576\n",
      "          E7       0.54      0.71      0.61      1176\n",
      "       EMaj7       0.72      0.63      0.67      4022\n",
      "       EMin7       0.77      0.62      0.69      6239\n",
      "         Eb7       0.54      0.77      0.64      1202\n",
      "      EbMaj7       0.70      0.67      0.69      3988\n",
      "      EbMin7       0.80      0.63      0.71      6240\n",
      "          F7       0.41      0.68      0.51      1165\n",
      "       FMaj7       0.69      0.71      0.70      3867\n",
      "       FMin7       0.70      0.70      0.70      6578\n",
      "          G7       0.42      0.83      0.56      1319\n",
      "       GMaj7       0.75      0.69      0.72      3851\n",
      "       GMin7       0.74      0.64      0.68      6224\n",
      "         Gb7       0.47      0.74      0.58      1150\n",
      "      GbMaj7       0.68      0.74      0.71      4003\n",
      "      GbMin7       0.67      0.75      0.71      6564\n",
      "\n",
      "    accuracy                           0.68    138891\n",
      "   macro avg       0.64      0.70      0.66    138891\n",
      "weighted avg       0.70      0.68      0.68    138891\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-31 05:23:07.575033: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 369005760 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12011/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3683 - loss: 2.4067"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-31 05:26:16.241717: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 235661760 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 18ms/step - accuracy: 0.3683 - loss: 2.4067 - val_accuracy: 0.5258 - val_loss: 1.6938\n",
      "Epoch 2/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 18ms/step - accuracy: 0.4970 - loss: 1.8052 - val_accuracy: 0.5401 - val_loss: 1.6271\n",
      "Epoch 3/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 18ms/step - accuracy: 0.5086 - loss: 1.7394 - val_accuracy: 0.5581 - val_loss: 1.5667\n",
      "Epoch 4/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 18ms/step - accuracy: 0.5147 - loss: 1.7087 - val_accuracy: 0.5565 - val_loss: 1.5567\n",
      "Epoch 5/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 18ms/step - accuracy: 0.5220 - loss: 1.6856 - val_accuracy: 0.5591 - val_loss: 1.5411\n",
      "Epoch 6/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 18ms/step - accuracy: 0.5249 - loss: 1.6752 - val_accuracy: 0.5630 - val_loss: 1.5355\n",
      "Epoch 7/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 18ms/step - accuracy: 0.5246 - loss: 1.6691 - val_accuracy: 0.5640 - val_loss: 1.5264\n",
      "Epoch 8/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 18ms/step - accuracy: 0.5271 - loss: 1.6571 - val_accuracy: 0.5584 - val_loss: 1.5422\n",
      "Epoch 9/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 18ms/step - accuracy: 0.5278 - loss: 1.6537 - val_accuracy: 0.5627 - val_loss: 1.5308\n",
      "Epoch 10/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 18ms/step - accuracy: 0.5299 - loss: 1.6499 - val_accuracy: 0.5709 - val_loss: 1.5053\n",
      "Epoch 11/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 18ms/step - accuracy: 0.5285 - loss: 1.6510 - val_accuracy: 0.5663 - val_loss: 1.5089\n",
      "Epoch 12/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 18ms/step - accuracy: 0.5307 - loss: 1.6415 - val_accuracy: 0.5711 - val_loss: 1.5074\n",
      "Epoch 13/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 18ms/step - accuracy: 0.5323 - loss: 1.6428 - val_accuracy: 0.5739 - val_loss: 1.4940\n",
      "Epoch 14/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 18ms/step - accuracy: 0.5329 - loss: 1.6357 - val_accuracy: 0.5751 - val_loss: 1.4902\n",
      "Epoch 15/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 18ms/step - accuracy: 0.5322 - loss: 1.6362 - val_accuracy: 0.5733 - val_loss: 1.4959\n",
      "Epoch 16/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 17ms/step - accuracy: 0.5340 - loss: 1.6310 - val_accuracy: 0.5676 - val_loss: 1.5059\n",
      "Epoch 17/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 18ms/step - accuracy: 0.5349 - loss: 1.6328 - val_accuracy: 0.5726 - val_loss: 1.4943\n",
      "Epoch 18/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 18ms/step - accuracy: 0.5359 - loss: 1.6256 - val_accuracy: 0.5726 - val_loss: 1.4968\n",
      "Epoch 19/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 18ms/step - accuracy: 0.5349 - loss: 1.6325 - val_accuracy: 0.5787 - val_loss: 1.4898\n",
      "Epoch 20/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 18ms/step - accuracy: 0.5375 - loss: 1.6224 - val_accuracy: 0.5801 - val_loss: 1.4813\n",
      "Epoch 21/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 17ms/step - accuracy: 0.5357 - loss: 1.6312 - val_accuracy: 0.5811 - val_loss: 1.4777\n",
      "Epoch 22/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 17ms/step - accuracy: 0.5376 - loss: 1.6178 - val_accuracy: 0.5821 - val_loss: 1.4696\n",
      "Epoch 23/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 17ms/step - accuracy: 0.5378 - loss: 1.6275 - val_accuracy: 0.5863 - val_loss: 1.4635\n",
      "Epoch 24/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 18ms/step - accuracy: 0.5384 - loss: 1.6253 - val_accuracy: 0.5824 - val_loss: 1.4742\n",
      "Epoch 25/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 18ms/step - accuracy: 0.5380 - loss: 1.6203 - val_accuracy: 0.5854 - val_loss: 1.4660\n",
      "Epoch 26/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 18ms/step - accuracy: 0.5391 - loss: 1.6164 - val_accuracy: 0.5770 - val_loss: 1.4848\n",
      "Epoch 27/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 18ms/step - accuracy: 0.5378 - loss: 1.6172 - val_accuracy: 0.5800 - val_loss: 1.4889\n",
      "Epoch 28/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 18ms/step - accuracy: 0.5383 - loss: 1.6205 - val_accuracy: 0.5832 - val_loss: 1.4758\n",
      "Epoch 29/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 18ms/step - accuracy: 0.5409 - loss: 1.6134 - val_accuracy: 0.5820 - val_loss: 1.4720\n",
      "Epoch 30/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 18ms/step - accuracy: 0.5417 - loss: 1.6105 - val_accuracy: 0.5787 - val_loss: 1.4852\n",
      "Epoch 31/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 19ms/step - accuracy: 0.5416 - loss: 1.6097 - val_accuracy: 0.5819 - val_loss: 1.4727\n",
      "Epoch 32/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 19ms/step - accuracy: 0.5402 - loss: 1.6113 - val_accuracy: 0.5824 - val_loss: 1.4737\n",
      "Epoch 33/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 20ms/step - accuracy: 0.5415 - loss: 1.6109 - val_accuracy: 0.5813 - val_loss: 1.4706\n",
      "::::: model_lstm_16_16_seq_20 :::::\n",
      "\u001b[1m4341/4341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.6145 - loss: 1.3681\n",
      "[1.4162863492965698, 0.6299493908882141]\n",
      "\u001b[1m4341/4341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A7       0.41      0.67      0.51      1177\n",
      "       AMaj7       0.64      0.64      0.64      4239\n",
      "       AMin7       0.66      0.59      0.62      6176\n",
      "         Ab7       0.39      0.64      0.49      1232\n",
      "      AbMaj7       0.64      0.62      0.63      4124\n",
      "      AbMin7       0.71      0.60      0.65      6631\n",
      "          B7       0.33      0.64      0.44      1163\n",
      "       BMaj7       0.70      0.64      0.67      3871\n",
      "       BMin7       0.68      0.61      0.64      6192\n",
      "         Bb7       0.38      0.60      0.46      1222\n",
      "      BbMaj7       0.71      0.58      0.64      3996\n",
      "      BbMin7       0.66      0.64      0.65      6313\n",
      "          C7       0.36      0.68      0.47      1193\n",
      "       CMaj7       0.64      0.67      0.65      3992\n",
      "       CMin7       0.71      0.64      0.68      6636\n",
      "          D7       0.39      0.65      0.49      1214\n",
      "       DMaj7       0.68      0.59      0.63      3896\n",
      "       DMin7       0.69      0.64      0.66      6272\n",
      "         Db7       0.51      0.57      0.54      1162\n",
      "      DbMaj7       0.67      0.62      0.64      4026\n",
      "      DbMin7       0.65      0.71      0.68      6576\n",
      "          E7       0.34      0.61      0.44      1176\n",
      "       EMaj7       0.70      0.61      0.65      4022\n",
      "       EMin7       0.76      0.54      0.63      6239\n",
      "         Eb7       0.40      0.64      0.49      1202\n",
      "      EbMaj7       0.62      0.69      0.65      3988\n",
      "      EbMin7       0.71      0.62      0.66      6240\n",
      "          F7       0.44      0.56      0.49      1165\n",
      "       FMaj7       0.66      0.64      0.65      3867\n",
      "       FMin7       0.68      0.64      0.66      6568\n",
      "          G7       0.39      0.72      0.50      1319\n",
      "       GMaj7       0.69      0.62      0.65      3851\n",
      "       GMin7       0.69      0.63      0.66      6224\n",
      "         Gb7       0.38      0.64      0.47      1150\n",
      "      GbMaj7       0.67      0.68      0.67      4003\n",
      "      GbMin7       0.70      0.67      0.68      6564\n",
      "\n",
      "    accuracy                           0.63    138881\n",
      "   macro avg       0.58      0.63      0.60    138881\n",
      "weighted avg       0.65      0.63      0.64    138881\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 19ms/step - accuracy: 0.4208 - loss: 2.2246 - val_accuracy: 0.5627 - val_loss: 1.5923\n",
      "Epoch 2/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 20ms/step - accuracy: 0.5355 - loss: 1.6787 - val_accuracy: 0.5771 - val_loss: 1.5250\n",
      "Epoch 3/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 20ms/step - accuracy: 0.5502 - loss: 1.6100 - val_accuracy: 0.5857 - val_loss: 1.4934\n",
      "Epoch 4/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 20ms/step - accuracy: 0.5573 - loss: 1.5807 - val_accuracy: 0.5933 - val_loss: 1.4663\n",
      "Epoch 5/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 21ms/step - accuracy: 0.5608 - loss: 1.5607 - val_accuracy: 0.5871 - val_loss: 1.4712\n",
      "Epoch 6/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 22ms/step - accuracy: 0.5647 - loss: 1.5415 - val_accuracy: 0.5970 - val_loss: 1.4475\n",
      "Epoch 7/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 22ms/step - accuracy: 0.5662 - loss: 1.5337 - val_accuracy: 0.5959 - val_loss: 1.4485\n",
      "Epoch 8/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 22ms/step - accuracy: 0.5694 - loss: 1.5271 - val_accuracy: 0.6012 - val_loss: 1.4334\n",
      "Epoch 9/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 23ms/step - accuracy: 0.5703 - loss: 1.5165 - val_accuracy: 0.6050 - val_loss: 1.4229\n",
      "Epoch 10/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 22ms/step - accuracy: 0.5728 - loss: 1.5112 - val_accuracy: 0.6080 - val_loss: 1.4114\n",
      "Epoch 11/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 22ms/step - accuracy: 0.5753 - loss: 1.5064 - val_accuracy: 0.6142 - val_loss: 1.3942\n",
      "Epoch 12/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 22ms/step - accuracy: 0.5762 - loss: 1.4998 - val_accuracy: 0.6134 - val_loss: 1.4062\n",
      "Epoch 13/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 22ms/step - accuracy: 0.5761 - loss: 1.4990 - val_accuracy: 0.5968 - val_loss: 1.4416\n",
      "Epoch 14/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 20ms/step - accuracy: 0.5765 - loss: 1.4903 - val_accuracy: 0.6092 - val_loss: 1.4048\n",
      "Epoch 15/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 20ms/step - accuracy: 0.5754 - loss: 1.4987 - val_accuracy: 0.5943 - val_loss: 1.4454\n",
      "Epoch 16/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 20ms/step - accuracy: 0.5764 - loss: 1.4961 - val_accuracy: 0.6140 - val_loss: 1.3949\n",
      "Epoch 17/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 20ms/step - accuracy: 0.5787 - loss: 1.4848 - val_accuracy: 0.6161 - val_loss: 1.3910\n",
      "Epoch 18/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 20ms/step - accuracy: 0.5771 - loss: 1.4920 - val_accuracy: 0.6043 - val_loss: 1.4271\n",
      "Epoch 19/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 20ms/step - accuracy: 0.5779 - loss: 1.4865 - val_accuracy: 0.6106 - val_loss: 1.4026\n",
      "Epoch 20/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 20ms/step - accuracy: 0.5774 - loss: 1.4910 - val_accuracy: 0.6120 - val_loss: 1.3999\n",
      "Epoch 21/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 20ms/step - accuracy: 0.5802 - loss: 1.4836 - val_accuracy: 0.6149 - val_loss: 1.3939\n",
      "Epoch 22/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 20ms/step - accuracy: 0.5776 - loss: 1.4936 - val_accuracy: 0.6198 - val_loss: 1.3738\n",
      "Epoch 23/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 20ms/step - accuracy: 0.5792 - loss: 1.4868 - val_accuracy: 0.6142 - val_loss: 1.3941\n",
      "Epoch 24/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m285s\u001b[0m 24ms/step - accuracy: 0.5801 - loss: 1.4823 - val_accuracy: 0.6153 - val_loss: 1.3916\n",
      "Epoch 25/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 25ms/step - accuracy: 0.5800 - loss: 1.4782 - val_accuracy: 0.6142 - val_loss: 1.3999\n",
      "Epoch 26/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 27ms/step - accuracy: 0.5796 - loss: 1.4838 - val_accuracy: 0.6087 - val_loss: 1.4113\n",
      "Epoch 27/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 29ms/step - accuracy: 0.5818 - loss: 1.4742 - val_accuracy: 0.6132 - val_loss: 1.3923\n",
      "Epoch 28/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 26ms/step - accuracy: 0.5806 - loss: 1.4772 - val_accuracy: 0.6128 - val_loss: 1.3950\n",
      "Epoch 29/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 28ms/step - accuracy: 0.5813 - loss: 1.4784 - val_accuracy: 0.6147 - val_loss: 1.3855\n",
      "Epoch 30/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 27ms/step - accuracy: 0.5807 - loss: 1.4689 - val_accuracy: 0.6180 - val_loss: 1.3781\n",
      "Epoch 31/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 26ms/step - accuracy: 0.5820 - loss: 1.4732 - val_accuracy: 0.6220 - val_loss: 1.3718\n",
      "Epoch 32/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 25ms/step - accuracy: 0.5823 - loss: 1.4724 - val_accuracy: 0.6230 - val_loss: 1.3724\n",
      "Epoch 33/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 26ms/step - accuracy: 0.5814 - loss: 1.4730 - val_accuracy: 0.6084 - val_loss: 1.4093\n",
      "Epoch 34/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 28ms/step - accuracy: 0.5814 - loss: 1.4701 - val_accuracy: 0.6190 - val_loss: 1.3669\n",
      "Epoch 35/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 26ms/step - accuracy: 0.5819 - loss: 1.4675 - val_accuracy: 0.6198 - val_loss: 1.3721\n",
      "Epoch 36/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 27ms/step - accuracy: 0.5833 - loss: 1.4730 - val_accuracy: 0.6173 - val_loss: 1.3874\n",
      "Epoch 37/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 26ms/step - accuracy: 0.5834 - loss: 1.4686 - val_accuracy: 0.6183 - val_loss: 1.3787\n",
      "Epoch 38/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 26ms/step - accuracy: 0.5817 - loss: 1.4687 - val_accuracy: 0.6232 - val_loss: 1.3504\n",
      "Epoch 39/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 26ms/step - accuracy: 0.5837 - loss: 1.4689 - val_accuracy: 0.6188 - val_loss: 1.3735\n",
      "Epoch 40/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 28ms/step - accuracy: 0.5829 - loss: 1.4633 - val_accuracy: 0.6192 - val_loss: 1.3752\n",
      "Epoch 41/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m363s\u001b[0m 30ms/step - accuracy: 0.5826 - loss: 1.4698 - val_accuracy: 0.6190 - val_loss: 1.3728\n",
      "Epoch 42/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 26ms/step - accuracy: 0.5822 - loss: 1.4675 - val_accuracy: 0.6229 - val_loss: 1.3588\n",
      "Epoch 43/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 28ms/step - accuracy: 0.5834 - loss: 1.4662 - val_accuracy: 0.6156 - val_loss: 1.3779\n",
      "Epoch 44/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 27ms/step - accuracy: 0.5831 - loss: 1.4625 - val_accuracy: 0.6266 - val_loss: 1.3482\n",
      "Epoch 45/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 28ms/step - accuracy: 0.5834 - loss: 1.4635 - val_accuracy: 0.6105 - val_loss: 1.4003\n",
      "Epoch 46/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 27ms/step - accuracy: 0.5834 - loss: 1.4668 - val_accuracy: 0.6205 - val_loss: 1.3717\n",
      "Epoch 47/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 27ms/step - accuracy: 0.5841 - loss: 1.4635 - val_accuracy: 0.6179 - val_loss: 1.3683\n",
      "Epoch 48/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 28ms/step - accuracy: 0.5824 - loss: 1.4637 - val_accuracy: 0.6250 - val_loss: 1.3536\n",
      "Epoch 49/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 28ms/step - accuracy: 0.5842 - loss: 1.4666 - val_accuracy: 0.6197 - val_loss: 1.3655\n",
      "Epoch 50/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 29ms/step - accuracy: 0.5828 - loss: 1.4612 - val_accuracy: 0.6244 - val_loss: 1.3579\n",
      "Epoch 51/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 27ms/step - accuracy: 0.5816 - loss: 1.4664 - val_accuracy: 0.6169 - val_loss: 1.3787\n",
      "Epoch 52/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 27ms/step - accuracy: 0.5849 - loss: 1.4618 - val_accuracy: 0.6151 - val_loss: 1.3762\n",
      "Epoch 53/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 27ms/step - accuracy: 0.5839 - loss: 1.4583 - val_accuracy: 0.6204 - val_loss: 1.3622\n",
      "Epoch 54/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 27ms/step - accuracy: 0.5851 - loss: 1.4556 - val_accuracy: 0.6235 - val_loss: 1.3563\n",
      "::::: model_lstm_32_32_seq_20 :::::\n",
      "\u001b[1m4341/4341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 7ms/step - accuracy: 0.6516 - loss: 1.2839\n",
      "[1.3195973634719849, 0.6623008251190186]\n",
      "\u001b[1m4341/4341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 7ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A7       0.47      0.73      0.57      1177\n",
      "       AMaj7       0.68      0.74      0.71      4239\n",
      "       AMin7       0.68      0.67      0.68      6176\n",
      "         Ab7       0.44      0.68      0.54      1232\n",
      "      AbMaj7       0.68      0.71      0.69      4124\n",
      "      AbMin7       0.71      0.63      0.67      6631\n",
      "          B7       0.51      0.66      0.57      1163\n",
      "       BMaj7       0.67      0.70      0.68      3871\n",
      "       BMin7       0.74      0.64      0.68      6192\n",
      "         Bb7       0.45      0.74      0.56      1222\n",
      "      BbMaj7       0.67      0.67      0.67      3996\n",
      "      BbMin7       0.70      0.66      0.68      6313\n",
      "          C7       0.49      0.68      0.57      1193\n",
      "       CMaj7       0.69      0.70      0.70      3992\n",
      "       CMin7       0.74      0.63      0.68      6636\n",
      "          D7       0.44      0.76      0.55      1214\n",
      "       DMaj7       0.68      0.68      0.68      3896\n",
      "       DMin7       0.74      0.64      0.69      6272\n",
      "         Db7       0.56      0.65      0.60      1162\n",
      "      DbMaj7       0.65      0.72      0.68      4026\n",
      "      DbMin7       0.62      0.74      0.68      6576\n",
      "          E7       0.37      0.80      0.51      1176\n",
      "       EMaj7       0.69      0.57      0.62      4022\n",
      "       EMin7       0.79      0.60      0.68      6239\n",
      "         Eb7       0.42      0.72      0.53      1202\n",
      "      EbMaj7       0.74      0.62      0.67      3988\n",
      "      EbMin7       0.71      0.64      0.67      6240\n",
      "          F7       0.36      0.69      0.47      1165\n",
      "       FMaj7       0.73      0.61      0.66      3867\n",
      "       FMin7       0.76      0.61      0.68      6568\n",
      "          G7       0.42      0.73      0.53      1319\n",
      "       GMaj7       0.68      0.74      0.71      3851\n",
      "       GMin7       0.69      0.66      0.67      6224\n",
      "         Gb7       0.48      0.76      0.59      1150\n",
      "      GbMaj7       0.73      0.66      0.69      4003\n",
      "      GbMin7       0.81      0.60      0.69      6564\n",
      "\n",
      "    accuracy                           0.66    138881\n",
      "   macro avg       0.62      0.68      0.64    138881\n",
      "weighted avg       0.68      0.66      0.67    138881\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 25ms/step - accuracy: 0.4519 - loss: 2.1537 - val_accuracy: 0.5838 - val_loss: 1.5825\n",
      "Epoch 2/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 25ms/step - accuracy: 0.5567 - loss: 1.6550 - val_accuracy: 0.5941 - val_loss: 1.5321\n",
      "Epoch 3/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 25ms/step - accuracy: 0.5692 - loss: 1.5943 - val_accuracy: 0.5986 - val_loss: 1.5023\n",
      "Epoch 4/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 27ms/step - accuracy: 0.5766 - loss: 1.5579 - val_accuracy: 0.6088 - val_loss: 1.4775\n",
      "Epoch 5/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 27ms/step - accuracy: 0.5786 - loss: 1.5465 - val_accuracy: 0.6159 - val_loss: 1.4559\n",
      "Epoch 6/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 27ms/step - accuracy: 0.5854 - loss: 1.5258 - val_accuracy: 0.6164 - val_loss: 1.4528\n",
      "Epoch 7/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 27ms/step - accuracy: 0.5870 - loss: 1.5191 - val_accuracy: 0.6168 - val_loss: 1.4487\n",
      "Epoch 8/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 27ms/step - accuracy: 0.5918 - loss: 1.4982 - val_accuracy: 0.6192 - val_loss: 1.4378\n",
      "Epoch 9/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 28ms/step - accuracy: 0.5925 - loss: 1.4952 - val_accuracy: 0.6176 - val_loss: 1.4470\n",
      "Epoch 10/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 28ms/step - accuracy: 0.5954 - loss: 1.4899 - val_accuracy: 0.6277 - val_loss: 1.4198\n",
      "Epoch 11/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 28ms/step - accuracy: 0.5971 - loss: 1.4791 - val_accuracy: 0.6330 - val_loss: 1.3945\n",
      "Epoch 12/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 28ms/step - accuracy: 0.5978 - loss: 1.4619 - val_accuracy: 0.6371 - val_loss: 1.3763\n",
      "Epoch 13/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 28ms/step - accuracy: 0.5988 - loss: 1.4640 - val_accuracy: 0.6206 - val_loss: 1.4262\n",
      "Epoch 14/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 28ms/step - accuracy: 0.5996 - loss: 1.4588 - val_accuracy: 0.6308 - val_loss: 1.4077\n",
      "Epoch 15/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m338s\u001b[0m 28ms/step - accuracy: 0.6009 - loss: 1.4559 - val_accuracy: 0.6369 - val_loss: 1.3851\n",
      "Epoch 16/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 27ms/step - accuracy: 0.6014 - loss: 1.4557 - val_accuracy: 0.6342 - val_loss: 1.3963\n",
      "Epoch 17/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 27ms/step - accuracy: 0.6030 - loss: 1.4470 - val_accuracy: 0.6410 - val_loss: 1.3586\n",
      "Epoch 18/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 28ms/step - accuracy: 0.6034 - loss: 1.4428 - val_accuracy: 0.6373 - val_loss: 1.3795\n",
      "Epoch 19/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 28ms/step - accuracy: 0.6047 - loss: 1.4489 - val_accuracy: 0.6440 - val_loss: 1.3684\n",
      "Epoch 20/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 28ms/step - accuracy: 0.6034 - loss: 1.4419 - val_accuracy: 0.6434 - val_loss: 1.3622\n",
      "Epoch 21/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m433s\u001b[0m 36ms/step - accuracy: 0.6047 - loss: 1.4420 - val_accuracy: 0.6408 - val_loss: 1.3670\n",
      "Epoch 22/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 37ms/step - accuracy: 0.6055 - loss: 1.4370 - val_accuracy: 0.6444 - val_loss: 1.3609\n",
      "Epoch 23/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m481s\u001b[0m 40ms/step - accuracy: 0.6062 - loss: 1.4322 - val_accuracy: 0.6436 - val_loss: 1.3546\n",
      "Epoch 24/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 28ms/step - accuracy: 0.6052 - loss: 1.4344 - val_accuracy: 0.6452 - val_loss: 1.3481\n",
      "Epoch 25/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m339s\u001b[0m 28ms/step - accuracy: 0.6055 - loss: 1.4326 - val_accuracy: 0.6503 - val_loss: 1.3344\n",
      "Epoch 26/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m344s\u001b[0m 29ms/step - accuracy: 0.6067 - loss: 1.4328 - val_accuracy: 0.6464 - val_loss: 1.3572\n",
      "Epoch 27/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 29ms/step - accuracy: 0.6068 - loss: 1.4328 - val_accuracy: 0.6522 - val_loss: 1.3273\n",
      "Epoch 28/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m350s\u001b[0m 29ms/step - accuracy: 0.6077 - loss: 1.4281 - val_accuracy: 0.6536 - val_loss: 1.3227\n",
      "Epoch 29/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 30ms/step - accuracy: 0.6072 - loss: 1.4216 - val_accuracy: 0.6459 - val_loss: 1.3447\n",
      "Epoch 30/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 30ms/step - accuracy: 0.6073 - loss: 1.4210 - val_accuracy: 0.6384 - val_loss: 1.3728\n",
      "Epoch 31/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 30ms/step - accuracy: 0.6068 - loss: 1.4228 - val_accuracy: 0.6538 - val_loss: 1.3319\n",
      "Epoch 32/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 30ms/step - accuracy: 0.6079 - loss: 1.4201 - val_accuracy: 0.6461 - val_loss: 1.3396\n",
      "Epoch 33/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m366s\u001b[0m 30ms/step - accuracy: 0.6073 - loss: 1.4147 - val_accuracy: 0.6445 - val_loss: 1.3503\n",
      "Epoch 34/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m366s\u001b[0m 30ms/step - accuracy: 0.6082 - loss: 1.4144 - val_accuracy: 0.6456 - val_loss: 1.3539\n",
      "Epoch 35/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m373s\u001b[0m 31ms/step - accuracy: 0.6083 - loss: 1.4157 - val_accuracy: 0.6464 - val_loss: 1.3445\n",
      "Epoch 36/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m372s\u001b[0m 31ms/step - accuracy: 0.6095 - loss: 1.4116 - val_accuracy: 0.6403 - val_loss: 1.3567\n",
      "Epoch 37/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m374s\u001b[0m 31ms/step - accuracy: 0.6094 - loss: 1.4114 - val_accuracy: 0.6506 - val_loss: 1.3297\n",
      "Epoch 38/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 31ms/step - accuracy: 0.6127 - loss: 1.4037 - val_accuracy: 0.6546 - val_loss: 1.3170\n",
      "Epoch 39/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 32ms/step - accuracy: 0.6108 - loss: 1.4050 - val_accuracy: 0.6387 - val_loss: 1.3645\n",
      "Epoch 40/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 32ms/step - accuracy: 0.6096 - loss: 1.4119 - val_accuracy: 0.6499 - val_loss: 1.3218\n",
      "Epoch 41/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 32ms/step - accuracy: 0.6106 - loss: 1.4009 - val_accuracy: 0.6411 - val_loss: 1.3576\n",
      "Epoch 42/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 32ms/step - accuracy: 0.6106 - loss: 1.4044 - val_accuracy: 0.6525 - val_loss: 1.3223\n",
      "Epoch 43/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 32ms/step - accuracy: 0.6123 - loss: 1.3998 - val_accuracy: 0.6476 - val_loss: 1.3340\n",
      "Epoch 44/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 33ms/step - accuracy: 0.6120 - loss: 1.3975 - val_accuracy: 0.6479 - val_loss: 1.3397\n",
      "Epoch 45/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m402s\u001b[0m 33ms/step - accuracy: 0.6106 - loss: 1.4022 - val_accuracy: 0.6508 - val_loss: 1.3336\n",
      "Epoch 46/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 33ms/step - accuracy: 0.6137 - loss: 1.3965 - val_accuracy: 0.6519 - val_loss: 1.3234\n",
      "Epoch 47/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m399s\u001b[0m 33ms/step - accuracy: 0.6111 - loss: 1.4014 - val_accuracy: 0.6535 - val_loss: 1.3225\n",
      "Epoch 48/500\n",
      "\u001b[1m12012/12012\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 33ms/step - accuracy: 0.6137 - loss: 1.3963 - val_accuracy: 0.6453 - val_loss: 1.3412\n",
      "::::: model_lstm_64_64_seq_20 :::::\n",
      "\u001b[1m4341/4341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 10ms/step - accuracy: 0.6648 - loss: 1.2907\n",
      "[1.3050286769866943, 0.6789049506187439]\n",
      "\u001b[1m4341/4341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 9ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A7       0.56      0.76      0.64      1177\n",
      "       AMaj7       0.75      0.65      0.69      4239\n",
      "       AMin7       0.79      0.60      0.68      6176\n",
      "         Ab7       0.47      0.75      0.58      1232\n",
      "      AbMaj7       0.69      0.63      0.66      4124\n",
      "      AbMin7       0.62      0.72      0.67      6631\n",
      "          B7       0.47      0.69      0.56      1163\n",
      "       BMaj7       0.78      0.49      0.60      3871\n",
      "       BMin7       0.74      0.64      0.69      6192\n",
      "         Bb7       0.37      0.87      0.52      1222\n",
      "      BbMaj7       0.70      0.72      0.71      3996\n",
      "      BbMin7       0.76      0.64      0.69      6313\n",
      "          C7       0.50      0.71      0.59      1193\n",
      "       CMaj7       0.67      0.75      0.71      3992\n",
      "       CMin7       0.72      0.70      0.71      6636\n",
      "          D7       0.55      0.77      0.64      1214\n",
      "       DMaj7       0.67      0.73      0.70      3896\n",
      "       DMin7       0.74      0.68      0.71      6272\n",
      "         Db7       0.52      0.77      0.62      1162\n",
      "      DbMaj7       0.72      0.67      0.70      4026\n",
      "      DbMin7       0.78      0.64      0.70      6576\n",
      "          E7       0.42      0.83      0.56      1176\n",
      "       EMaj7       0.61      0.70      0.66      4022\n",
      "       EMin7       0.71      0.67      0.69      6239\n",
      "         Eb7       0.49      0.83      0.62      1202\n",
      "      EbMaj7       0.68      0.69      0.68      3988\n",
      "      EbMin7       0.74      0.64      0.69      6240\n",
      "          F7       0.60      0.80      0.69      1165\n",
      "       FMaj7       0.75      0.66      0.70      3867\n",
      "       FMin7       0.69      0.71      0.70      6568\n",
      "          G7       0.56      0.73      0.63      1319\n",
      "       GMaj7       0.66      0.76      0.71      3851\n",
      "       GMin7       0.69      0.71      0.70      6224\n",
      "         Gb7       0.54      0.75      0.63      1150\n",
      "      GbMaj7       0.76      0.63      0.69      4003\n",
      "      GbMin7       0.81      0.62      0.70      6564\n",
      "\n",
      "    accuracy                           0.68    138881\n",
      "   macro avg       0.65      0.70      0.66    138881\n",
      "weighted avg       0.70      0.68      0.68    138881\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    (16, 16),\n",
    "    (32, 32),\n",
    "    (64, 64)\n",
    "]\n",
    "\n",
    "seq_lens = [5, 10, 20]\n",
    "    \n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    # monitor=\"val_accuracy\"\n",
    ")\n",
    "\n",
    "def make_seq(X, y_encoded, seq_len):\n",
    "    X_seq, y_encoded_seq = None, None\n",
    "    X_seq_list = []\n",
    "    y_encoded_seq_list = []\n",
    "    for i in range(len(X) - seq_len + 1):\n",
    "        X_seq_list.append(X.values[i : i + seq_len, :])\n",
    "        y_encoded_seq_list.append(y_encoded[i + seq_len - 1])\n",
    "    \n",
    "    return np.array(X_seq_list), np.array(y_encoded_seq_list)\n",
    "\n",
    "# models = []\n",
    "for seq_len in seq_lens:\n",
    "    X_train_seq, y_train_encoded_seq = make_seq(X_train, y_train_encoded, seq_len)\n",
    "    X_val_seq, y_val_encoded_seq = make_seq(X_val, y_val_encoded, seq_len)\n",
    "    X_test_seq, y_test_encoded_seq = make_seq(X_test, y_test_encoded, seq_len)\n",
    "    \n",
    "    for layer_1, layer_2 in layers:\n",
    "        model_lstm = tf.keras.Sequential([\n",
    "            # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True, input_shape=(SEQUENCE_LEN, X_seq_train.shape[2]))),\n",
    "            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
    "                layer_1,\n",
    "                return_sequences=True,\n",
    "                input_shape=(seq_len, X_train_seq.shape[2]),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(1e-3),\n",
    "                recurrent_regularizer=tf.keras.regularizers.l2(1e-3),\n",
    "                recurrent_dropout=0.25,\n",
    "            )),\n",
    "            tf.keras.layers.Dropout(0.25),\n",
    "                \n",
    "            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
    "                layer_2,\n",
    "                return_sequences=False,\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(1e-3),\n",
    "                recurrent_regularizer=tf.keras.regularizers.l2(1e-3),\n",
    "                recurrent_dropout=0.25,\n",
    "            )),\n",
    "            tf.keras.layers.Dropout(0.25),\n",
    "        \n",
    "            tf.keras.layers.Dense(\n",
    "                len(encoder.classes_),\n",
    "                activation='softmax',\n",
    "                # kernel_regularizer=tf.keras.regularizers.l2(1e-2)\n",
    "            ),\n",
    "        ])\n",
    "        \n",
    "        model_lstm.compile(\n",
    "            optimizer=\"adam\",\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        \n",
    "        # model_lstm.summary()\n",
    "        \n",
    "        history = model_lstm.fit(\n",
    "            # X_seq_train,\n",
    "            # y_seq_train,\n",
    "            X_train_seq,\n",
    "            y_train_encoded_seq,\n",
    "            epochs=500,\n",
    "            batch_size=32,\n",
    "            # validation_split=0.1,\n",
    "            validation_data=(X_val_seq, y_val_encoded_seq),\n",
    "            verbose=1,\n",
    "            callbacks=[early_stopping],\n",
    "            class_weight=class_weight_dict,\n",
    "        )\n",
    "    \n",
    "        # models.append(model_lstm)\n",
    "        \n",
    "        model_lstm.save(f\"./Models/model_lstm_{layer_1}_{layer_2}_seq_{seq_len}.keras\")\n",
    "        pd.DataFrame(history.history).to_csv(f\"./History/model_lstm_{layer_1}_{layer_2}_seq_{seq_len}_history.csv\", index=False)\n",
    "\n",
    "        print(f\"::::: model_lstm_{layer_1}_{layer_2}_seq_{seq_len} :::::\")\n",
    "        print(model_lstm.evaluate(X_test_seq, y_test_encoded_seq))\n",
    "        y_pred = np.argmax(model_lstm.predict(X_test_seq), axis=1)\n",
    "        print(\n",
    "            sk.metrics.classification_report(y_test_encoded_seq, y_pred, target_names=encoder.classes_)\n",
    "        )\n",
    "        print(\"\")\n",
    "    \n",
    "        gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_venv",
   "language": "python",
   "name": "thesis_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
